/*
 * Page Coherence Management for CXL Shared Memory
 *
 * This file implements page coherence functionality for managing 
 * replica pages in CXL shared memory environments.
 */

#include <linux/mm.h>
#include <linux/pgtable.h>
#include <linux/slab.h>
#include <linux/pfn_t.h>
#include <swmc/page_coherence.h>
#include <linux/highmem.h>
#include <linux/vmalloc.h>
#include <linux/iomap.h>
#include <linux/gfp.h>
#include <linux/printk.h>
#include <linux/memcontrol.h>
#include <linux/hugetlb.h>    /* pfn_pmd, pmd_mkdirty, set_pmd_at */
#include <swmc/page_replication_info.h>
#include <linux/xarray.h>
#include <linux/completion.h>
#include <linux/spinlock.h>
#include <swmc/swmc_kmsg.h>
#include "wait_station.h"
#include <linux/rmap.h>
#include <linux/pagewalk.h>
#include <linux/mm_inline.h>
#include <linux/pagemap.h>
#include <linux/fs.h>
#include <linux/atomic.h>
#include <linux/shmem_fs.h>
#include <linux/kobject.h>
#include <linux/sysfs.h>

#ifdef CONFIG_PAGE_COHERENCE

/*
 * Page Coherence Management for CXL Shared Memory
 *
 * This file implements page coherence functionality for managing
 * replica pages in CXL shared memory environments.
 */

// dummy base PA for CXL HDM 
static unsigned long cxl_hdm_base = 0x1e80000000; // Default value, can be set by external module

/* Page coherence fault statistics */
static atomic64_t page_coherence_fault_count = ATOMIC64_INIT(0);
static atomic64_t page_coherence_fault_read_count = ATOMIC64_INIT(0);
static atomic64_t page_coherence_fault_write_count = ATOMIC64_INIT(0);
static atomic64_t page_coherence_replica_found_count = ATOMIC64_INIT(0);
static atomic64_t page_coherence_replica_created_count = ATOMIC64_INIT(0);

/**
 * set_cxl_hdm_base - Set the CXL HDM base address
 * @base_addr: CXL HDM base physical address
 *
 * This function allows external modules to set the CXL HDM base address
 * during their initialization phase.
 */
void set_cxl_hdm_base(unsigned long base_addr)
{
    cxl_hdm_base = base_addr;
    pr_info("[page_coherence] CXL HDM base address set to 0x%lx\n", base_addr);
}
EXPORT_SYMBOL(set_cxl_hdm_base);

/**
 * get_cxl_hdm_base - Get the current CXL HDM base address
 *
 * Returns: Current CXL HDM base address
 */
unsigned long get_cxl_hdm_base(void)
{
    return cxl_hdm_base;
}
EXPORT_SYMBOL(get_cxl_hdm_base);

/* =============================================================================
 * SYSFS INTERFACE FOR PAGE COHERENCE FAULT STATISTICS
 * ============================================================================= */

/* Sysfs show functions for fault statistics */
static ssize_t fault_count_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)
{
    return sprintf(buf, "%llu\n", atomic64_read(&page_coherence_fault_count));
}

static ssize_t fault_read_count_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)
{
    return sprintf(buf, "%llu\n", atomic64_read(&page_coherence_fault_read_count));
}

static ssize_t fault_write_count_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)
{
    return sprintf(buf, "%llu\n", atomic64_read(&page_coherence_fault_write_count));
}

static ssize_t replica_found_count_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)
{
    return sprintf(buf, "%llu\n", atomic64_read(&page_coherence_replica_found_count));
}

static ssize_t replica_created_count_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)
{
    return sprintf(buf, "%llu\n", atomic64_read(&page_coherence_replica_created_count));
}

/* Sysfs store function for resetting counters */
static ssize_t reset_counters_store(struct kobject *kobj, struct kobj_attribute *attr,
                                  const char *buf, size_t count)
{
    int reset_value;
    
    if (kstrtoint(buf, 10, &reset_value) < 0)
        return -EINVAL;
    
    if (reset_value == 1) {
        atomic64_set(&page_coherence_fault_count, 0);
        atomic64_set(&page_coherence_fault_read_count, 0);
        atomic64_set(&page_coherence_fault_write_count, 0);
        atomic64_set(&page_coherence_replica_found_count, 0);
        atomic64_set(&page_coherence_replica_created_count, 0);
        pr_info("[page_coherence] All fault counters reset\n");
    }
    
    return count;
}

/* Define sysfs attributes */
static struct kobj_attribute fault_count_attr = __ATTR_RO(fault_count);
static struct kobj_attribute fault_read_count_attr = __ATTR_RO(fault_read_count);
static struct kobj_attribute fault_write_count_attr = __ATTR_RO(fault_write_count);
static struct kobj_attribute replica_found_count_attr = __ATTR_RO(replica_found_count);
static struct kobj_attribute replica_created_count_attr = __ATTR_RO(replica_created_count);
static struct kobj_attribute reset_counters_attr = __ATTR_WO(reset_counters);

/* Array of attributes for the attribute group */
static struct attribute *page_coherence_attrs[] = {
    &fault_count_attr.attr,
    &fault_read_count_attr.attr,
    &fault_write_count_attr.attr,
    &replica_found_count_attr.attr,
    &replica_created_count_attr.attr,
    &reset_counters_attr.attr,
    NULL,
};

static struct attribute_group page_coherence_attr_group = {
    .name = "page_coherence",
    .attrs = page_coherence_attrs,
};

static struct kobject *page_coherence_kobj;

/* =============================================================================
 * MESSAGE HANDLING FUNCTIONS
 * ============================================================================= */

// Fetch message handling
// M-> S, S -> S, I -> I
static int swmc_kmsg_handle_fetch(struct swmc_kmsg_message *msg)
{
    int ret = 0;
    struct payload_data *payload = &msg->payload;
    // Validate message
    if (!msg || msg->header.type != SWMC_KMSG_TYPE_FETCH) {
        pr_err("[page_coherence] Invalid fetch message\n");
        return -EINVAL;
    }

    pr_info("[page_coherence] Handling fetch message for offset 0x%lx\n", payload->cxl_hdm_offset);

    // calculate original pfn from payload->cxl_hdm_offset
    unsigned long original_phys_addr = cxl_hdm_base + payload->cxl_hdm_offset;
    pfn_t original_pfn;

    if (payload->page_order == 0) {
        original_pfn = pfn_to_pfn_t(original_phys_addr >> PAGE_SHIFT);
    }
    else if (payload->page_order == PMD_ORDER) {
        original_pfn = pfn_to_pfn_t(original_phys_addr >> PMD_SHIFT);
    } else {
        pr_err("[page_coherence] Invalid page order: %d\n", payload->page_order);
        return -EINVAL;
    }

    // find replica page from original pfn
    struct page *replica_page = replica_page_find(original_pfn);

    if (!replica_page) { // I -> I
        pr_info("[page_coherence] No replica page found (I->I transition)\n");
    } else { // M -> S, S -> S
        pr_info("[page_coherence] Replica page found (M->S or S->S transition)\n");
        
        /* For M->S transition: remove write permissions from all mappings
         * For S->S transition: already read-only, but ensure consistency
         */
        int protected_mappings = replica_page_write_protect(replica_page);
        if (protected_mappings > 0) {
            pr_info("[page_coherence] M->S transition: write-protected %d mappings\n", protected_mappings);
        } else if (protected_mappings == 0) {
            pr_info("[page_coherence] S->S transition: page already read-only\n");
        } else {
            pr_info("[page_coherence] Failed to write-protect replica page: %d\n", protected_mappings);
        }
        
        /* Release reference obtained from replica_page_find() */
        replica_page_put(replica_page);
    }
    // send back fetch_ack_message using payload of original kmsg
    ret = swmc_kmsg_unicast(SWMC_KMSG_TYPE_FETCH_ACK, msg->header.ws_id, msg->header.from_nid, payload);

    return 0;
}

// Fetch_ack message handling
static int swmc_kmsg_handle_fetch_ack(struct swmc_kmsg_message *msg)
{
    // check if the message is valid
    if (!msg || msg->header.type != SWMC_KMSG_TYPE_FETCH_ACK) {
        pr_err("[page_coherence] Invalid fetch_ack message\n");
        return -EINVAL;
    }

    pr_info("[page_coherence] Handling fetch_ack message for offset 0x%lx\n", msg->payload.cxl_hdm_offset);

    // find the wait station by ID
    struct wait_station *ws = wait_station(msg->header.ws_id);
    if (!ws) {
        pr_err("[page_coherence] Invalid wait station ID: %d\n", msg->header.ws_id);
        return -EINVAL;
    }

    // Decrease pending count atomically
    if (atomic_dec_and_test(&ws->pendings_count)) {
        // All fetch ACKs received, wake up the wait station
        pr_info("[page_coherence] All fetch ACKs received for wait station %d\n", msg->header.ws_id);
        complete(&ws->pendings);
    } else {
        pr_info("[page_coherence] Fetch ACK received, pending count: %d\n",
                atomic_read(&ws->pendings_count));
    }

    return 0;
}

// Invalidate message handling
// M -> I, S -> I, I -> I (M -> I is violated)
static int swmc_kmsg_handle_invalidate(struct swmc_kmsg_message *msg)
{
    // Validate message
    if (!msg || msg->header.type != SWMC_KMSG_TYPE_INVALIDATE) {
        pr_err("[page_coherence] Invalid invalidate message\n");
        return -EINVAL;
    }

    struct payload_data *payload = &msg->payload;
    int ret = 0;

    pr_info("[page_coherence] Handling invalidate message for offset 0x%lx\n", payload->cxl_hdm_offset);
    
    // calculate original pfn from payload->cxl_hdm_offset
    unsigned long original_phys_addr = cxl_hdm_base + payload->cxl_hdm_offset;
    pfn_t original_pfn;

    if (payload->page_order == 0) {
        original_pfn = pfn_to_pfn_t(original_phys_addr >> PAGE_SHIFT);
    }
    else if (payload->page_order == PMD_ORDER) {
        original_pfn = pfn_to_pfn_t(original_phys_addr >> PMD_SHIFT);
    } else {
        pr_err("[page_coherence] Invalid page order: %d\n", payload->page_order);
        return -EINVAL;
    }

    // find replica page from original pfn
    struct page *replica_page = replica_page_find(original_pfn);

    if (!replica_page) { // I -> I
        pr_info("[page_coherence] No replica page found (I->I transition)\n");
    }
    else { // M -> I, S -> I
        pr_info("[page_coherence] Replica page found (M->I or S->I transition)\n");
        
        /* For M->I or S->I transitions, clean up the replica page
         * This includes flushing dirty data back to original and freeing the replica
         */
        int invalidate_ret = replica_page_flush(replica_page);
        if (invalidate_ret) {
            pr_info("[page_coherence] Failed to flush replica page: %d\n", invalidate_ret);
        } else {
            pr_info("[page_coherence] Successfully flushed replica page\n");
        }
        
        /* Release reference obtained from replica_page_find() */
        replica_page_put(replica_page);
    }

    // send back invalidate_ack_message using payload of original kmsg
    ret = swmc_kmsg_unicast(SWMC_KMSG_TYPE_INVALIDATE_ACK, msg->header.ws_id, msg->header.from_nid, payload);

    return 0;
}

// Invalidate_ack message handling
static int swmc_kmsg_handle_invalidate_ack(struct swmc_kmsg_message *msg)
{
    // check if the message is valid
    if (!msg || msg->header.type != SWMC_KMSG_TYPE_INVALIDATE_ACK) {
        pr_err("[page_coherence] Invalid invalidate_ack message\n");
        return -EINVAL;
    }

    pr_info("[page_coherence] Handling invalidate_ack message for offset 0x%lx\n", msg->payload.cxl_hdm_offset);

    // find the wait station by ID
    struct wait_station *ws = wait_station(msg->header.ws_id);
    if (!ws) {
        pr_err("[page_coherence] Invalid wait station ID: %d\n", msg->header.ws_id);
        return -EINVAL;
    }

    // Decrease pending count atomically
    if (atomic_dec_and_test(&ws->pendings_count)) {
        // All invalidate ACKs received, wake up the wait station
        pr_info("[page_coherence] All invalidate ACKs received for wait station %d\n", msg->header.ws_id);
        complete(&ws->pendings);
    } else {
        pr_info("[page_coherence] Invalidate ACK received, pending count: %d\n",
                atomic_read(&ws->pendings_count));
    }

    return 0;
}

// Error message handling
static int swmc_kmsg_handle_error(struct swmc_kmsg_message *msg)
{
    // Validate message
    if (!msg || msg->header.type != SWMC_KMSG_TYPE_ERROR) {
        pr_err("[page_coherence] Invalid error message\n");
        return -EINVAL;
    }

    pr_err("[page_coherence] Received error message from node %d for offset 0x%lx\n",
           msg->header.from_nid, msg->payload.cxl_hdm_offset);

    // TODO: Handle error appropriately
    return 0;
}


/* =============================================================================
 * PAGE COHERENCE FAULT HANDLING
 * ============================================================================= */
/* Helper function to handle messaging operations */
static int broadcast_message_and_wait(enum swmc_kmsg_type msg_type, 
                                     int node_count, struct payload_data *payload,
                                     unsigned long cxl_hdm_offset)
{
    struct wait_station *ws;
    int ret;

    // register wait station for this fault
    ws = get_wait_station_multiple(current, node_count - 1);
    if (!ws) {
        pr_info("[page_coherence] Failed to get wait station\n");
        return -ENOMEM;
    }
    
    // broadcast message
    ret = swmc_kmsg_broadcast(msg_type, ws->id, payload);
    if (ret) {
        pr_info("[page_coherence] Failed to send %s message: %d\n", 
               msg_type == SWMC_KMSG_TYPE_FETCH ? "fetch" : "invalidate", ret);
        // Continue anyway for now - could implement fallback
    }

    void *wait_result = wait_at_station(ws);
    if (IS_ERR(wait_result)) {
        ret = PTR_ERR(wait_result);
        pr_info("[page_coherence] Failed to wait at station: %d\n", ret);
        return ret;
    }

    pr_info("[page_coherence] Waited at station, received response for %s message\n",
            msg_type == SWMC_KMSG_TYPE_FETCH ? "fetch" : "invalidate");
    return 0;
}

/* Helper function to handle PMD reuse case */
/* TODO: Remove if not needed after testing new logic
static int handle_pmd_reuse(pmd_t pmd_val, pfn_t original_pfn, pfn_t *pfn)
{
    pfn_t replica_pfn;
    
    pr_info("[page_coherence] Skipping replication and copying for write fault on non-writable PMD.\n");
    replica_pfn.val = pmd_pfn(pmd_val) | (original_pfn.val & PFN_FLAGS_MASK);
    *pfn = replica_pfn;
    pr_info("[page_coherence] Updated pfn to existing PMD PFN: %lu\n",
            pfn_t_to_pfn(replica_pfn));
    return 0;
}
*/

/**
 * page_coherence_fault - Handle page coherence faults
 * @vmf: Fault information structure
 * @iter: IOMAP iterator
 * @size: Size of the fault (PAGE_SIZE or PMD_SIZE)
 * @kaddr: Kernel virtual address of the original page
 * @pfn: Pointer to the page frame number, will be updated to replica PFN
 *
 * Returns: 0 on success, negative error code on failure
 */
int page_coherence_fault(struct vm_fault *vmf, const struct iomap_iter *iter,
                         size_t size, void *kaddr, pfn_t *pfn)
{
    struct page *replica_page;
    unsigned int order;
    pfn_t original_pfn = *pfn;
    pfn_t replica_pfn;
    int ret;
    bool write = iter->flags & IOMAP_WRITE;
    // bool leader; // is this fault is the leader fault of this cxl_hdm_offset?
    struct payload_data payload;
    /* TODO: Remove if not needed after testing new logic
    pmd_t *pmd;
    pmd_t pmd_val;
    */
    int node_count;
    unsigned long cxl_hdm_offset;
    bool need_invalidate = false;

    /* Increment total fault counter */
    atomic64_inc(&page_coherence_fault_count);
    
    /* Increment read/write specific counters */
    if (write) {
        atomic64_inc(&page_coherence_fault_write_count);
    } else {
        atomic64_inc(&page_coherence_fault_read_count);
    }

    if (!vmf || !iter || !kaddr || !pfn) {
        pr_err("[%s] Invalid parameters\n", __func__);
        return -EINVAL;
    }

    /* Determine page order */
    if (size == PMD_SIZE)
        order = PMD_ORDER;
    else if (size == PAGE_SIZE)
        order = 0;
    else {
        pr_err("[%s] Unsupported size: %zu\n", __func__, size);
        return -EINVAL;
    }

    pr_info("[%s] Fault at 0x%lx: size=%zu order=%u original pfn=%lu\n",
        __func__, vmf->address, size, order, pfn_t_to_pfn(original_pfn));

    // Get CXL HDM offset for this fault
    cxl_hdm_offset = pfn_t_to_pfn(original_pfn) * PAGE_SIZE - cxl_hdm_base;
    
    // TODO: check if this is the leader fault for this cxl_hdm_offset. for now, assume every fault is a leader
    // leader = true;

    payload.cxl_hdm_offset = cxl_hdm_offset;
    payload.page_order = order;

    node_count = swmc_kmsg_node_count();
    if (node_count <= 0) {
        pr_err("[%s] Invalid node count: %d\n", __func__, node_count);
        return -EINVAL;
    }

    /* =======================================================================
     * STEP 1: Check if replica already exists for this original PFN
     * ======================================================================= */
    
    /* First, check if a replica page already exists for this original PFN */
    replica_page = replica_page_find(original_pfn);
    
    if (replica_page) {
        /* Replica exists - handle different fault scenarios */
        atomic64_inc(&page_coherence_replica_found_count);
        pr_info("[%s] Replica page found: %p\n", __func__, replica_page);
        
        if (!write) {
            /* READ FAULT on existing replica:
             * Simply map the existing replica page without any protocol messages.
             * This handles the case where multiple processes read the same page.
             */
            pr_info("[%s] Read fault on existing replica - direct mapping\n", __func__);

            /* Build replica PFN with preserved flags and return */
            replica_pfn.val = page_to_pfn(replica_page) |
                              (original_pfn.val & PFN_FLAGS_MASK);
            *pfn = replica_pfn;

            pr_info("[%s] Mapped existing replica pfn=%lu at 0x%lx\n",
                    __func__, pfn_t_to_pfn(replica_pfn), vmf->address);
            
            ret = 0;
            goto put_replica_and_return;
        } else {
            /* WRITE FAULT on existing replica:
             * Need to check if replica is already writable by ANY process in the system.
             * If already writable somewhere, this is M->M (just map it writable here too).
             * If only readable, this is S->M transition (need invalidation to others).
             */
            pr_info("[%s] Write fault on existing replica\n", __func__);

            /* Check if the replica page is currently mapped as writable anywhere in the system.
             * We use page_mapped() and check if any PTE/PMD has write permissions.
             * This tells us the current coherence state:
             * - If writable mappings exist: page is in M state
             * - If only read-only mappings exist: page is in S state
             */
            bool is_currently_writable = false;
            
            if (page_mapped(replica_page)) {
                /* Use folio operations to check if page has writable mappings */
                struct folio *folio = page_folio(replica_page);
                
                /* Check if this page is mapped writable anywhere in the system.
                 * We can use page_mkclean() return value to determine this:
                 * - If folio_mkclean() returns > 0: had writable mappings (M state)
                 * - If folio_mkclean() returns 0: no writable mappings (S state)
                 * But we don't want to actually clean it, so we need another approach.
                 */
                
                /* Alternative: Check if page is dirty, which indicates recent writes */
                if (folio_test_dirty(folio)) {
                    is_currently_writable = true;
                    pr_info("[%s] Replica page is dirty - likely in M state\n", __func__);
                } else {
                    /* Page is clean - likely in S state, but could be M with clean data */
                    pr_info("[%s] Replica page is clean - likely in S state\n", __func__);
                    is_currently_writable = false;
                }
            } else {
                /* Page not mapped anywhere - shouldn't happen for existing replica */
                pr_warn("[%s] Replica exists but not mapped anywhere\n", __func__);
                is_currently_writable = false;
            }
            
            if (is_currently_writable) {
                /* Replica is already in M state - just map it writable here too (M->M) */
                pr_info("[%s] Replica already writable elsewhere - M->M transition\n", __func__);
                
                /* No protocol messages needed - just map the existing replica as writable */
                replica_pfn.val = page_to_pfn(replica_page) |
                                  (original_pfn.val & PFN_FLAGS_MASK);
                *pfn = replica_pfn;

                pr_info("[%s] Mapped existing writable replica pfn=%lu at 0x%lx\n",
                        __func__, pfn_t_to_pfn(replica_pfn), vmf->address);
                
                ret = 0;
                goto put_replica_and_return;
            } else {
                /* Replica is in S state - need S->M transition */
                pr_info("[%s] S->M transition needed for existing replica\n", __func__);

                /* Send invalidation to other nodes for S->M transition */
                ret = broadcast_message_and_wait(SWMC_KMSG_TYPE_INVALIDATE, node_count, 
                                               &payload, cxl_hdm_offset);
                if (ret)
                    goto put_replica_and_return;
                
                /* Map the existing replica as writable */
                replica_pfn.val = page_to_pfn(replica_page) |
                                  (original_pfn.val & PFN_FLAGS_MASK);
                *pfn = replica_pfn;

                pr_info("[%s] Mapped existing replica as writable pfn=%lu at 0x%lx\n",
                        __func__, pfn_t_to_pfn(replica_pfn), vmf->address);
                
                ret = 0;
                goto put_replica_and_return;
            }
        }
    }
    
    /* =======================================================================
     * STEP 2: No replica exists - create new replica (I->S or I->M)
     * ======================================================================= */

    pr_info("[%s] No replica found - creating new replica\n", __func__);

    /* Determine what protocol messages are needed for new replica creation */
    if (write) {
        /* I->M transition: Need both fetch and invalidate */
        pr_info("[%s] I->M transition: need fetch + invalidate\n", __func__);
        need_invalidate = true;
    } else {
        /* I->S transition: Only need fetch */
        pr_info("[%s] I->S transition: need fetch only\n", __func__);
        need_invalidate = false;
    }
    
    /* Send fetch message to other nodes */
    ret = broadcast_message_and_wait(SWMC_KMSG_TYPE_FETCH, node_count, 
                                   &payload, cxl_hdm_offset);
    if (ret)
        return ret;
    
    /* Send invalidate message if needed (for I->M transition) */
    if (need_invalidate) {
        pr_info("[%s] Broadcasting invalidate message for I->M transition\n", __func__);
        ret = broadcast_message_and_wait(SWMC_KMSG_TYPE_INVALIDATE, node_count, 
                                       &payload, cxl_hdm_offset);
        if (ret)
            return ret;
    }

    /* Create replica page using page_replication.c API */
    replica_page = replica_page_create(order, original_pfn, kaddr, size);
    if (IS_ERR(replica_page)) {
        ret = PTR_ERR(replica_page);
        pr_info("[%s] Failed to create replica page: %d\n", __func__, ret);
        return ret;
    }
    
    /* Increment replica created counter */
    atomic64_inc(&page_coherence_replica_created_count);
    
    // Validate original page exists for devdax
    struct page *original_page_ptr = pfn_t_to_page(original_pfn);
    if (!original_page_ptr) {
        pr_info("[%s] Invalid original page pointer\n", __func__);
        /* Clean up the replica page we just created */
        replica_page_unmap_and_free(replica_page);
        return -EINVAL;
    }
    pr_info("[%s] Original page: %p, replica page: %p\n", __func__, original_page_ptr, replica_page);

    /* Build new PFN with preserved flags and update */
    replica_pfn.val = page_to_pfn(replica_page) |
                      (original_pfn.val & PFN_FLAGS_MASK);
    *pfn = replica_pfn;

    pr_info("[%s] Replicated pfn=%lu at 0x%lx\n",
            __func__, pfn_t_to_pfn(replica_pfn), vmf->address);
    return 0;

put_replica_and_return:
    /* Release reference obtained from replica_page_find() */
    replica_page_put(replica_page);
    return ret;
}

/**
 * page_coherence_init - Initialize page coherence subsystem
 *
 * Returns: 0 on success, negative error code on failure
 */
int __init page_coherence_init(void)
{
    int ret;
    
    pr_info("[page_coherence] Initializing page coherence subsystem\n");

    // Register message handlers
    ret = swmc_kmsg_register_callback(SWMC_KMSG_TYPE_FETCH, swmc_kmsg_handle_fetch);
    if (ret) {
        pr_info("[page_coherence] Failed to register fetch handler: %d\n", ret);
        return ret;
    }

    ret = swmc_kmsg_register_callback(SWMC_KMSG_TYPE_INVALIDATE, swmc_kmsg_handle_invalidate);
    if (ret) {
        pr_info("[page_coherence] Failed to register invalidate handler: %d\n", ret);
        return ret;
    }

    ret = swmc_kmsg_register_callback(SWMC_KMSG_TYPE_FETCH_ACK, swmc_kmsg_handle_fetch_ack);
    if (ret) {
        pr_info("[page_coherence] Failed to register fetch_ack handler: %d\n", ret);
        return ret;
    }

    ret = swmc_kmsg_register_callback(SWMC_KMSG_TYPE_INVALIDATE_ACK, swmc_kmsg_handle_invalidate_ack);
    if (ret) {
        pr_info("[page_coherence] Failed to register invalidate_ack handler: %d\n", ret);
        return ret;
    }

    ret = swmc_kmsg_register_callback(SWMC_KMSG_TYPE_ERROR, swmc_kmsg_handle_error);
    if (ret) {
        pr_info("[page_coherence] Failed to register error handler: %d\n", ret);
        return ret;
    }

    /* Create sysfs interface for fault statistics */
    page_coherence_kobj = kobject_create_and_add("swmc", kernel_kobj);
    if (!page_coherence_kobj) {
        pr_err("[page_coherence] Failed to create kobject\n");
        return -ENOMEM;
    }

    ret = sysfs_create_group(page_coherence_kobj, &page_coherence_attr_group);
    if (ret) {
        pr_err("[page_coherence] Failed to create sysfs group: %d\n", ret);
        kobject_put(page_coherence_kobj);
        return ret;
    }

    pr_info("[page_coherence] Page coherence subsystem initialized successfully\n");
    pr_info("[page_coherence] Sysfs interface available at /sys/kernel/swmc/page_coherence/\n");
    return 0;
}

subsys_initcall(page_coherence_init);

#endif /* CONFIG_PAGE_COHERENCE */
