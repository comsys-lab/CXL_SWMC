// SPDX-License-Identifier: GPL-2.0
// Replica pages: LRU with **hardware accessed (young) bit** via folio_referenced(), plus pr_info debug logs only.

#include <linux/list.h>
#include <linux/rmap.h>
#include <linux/swap.h>
#include <linux/mm.h>
#include <linux/highmem.h>
#include <linux/gfp.h>
#include <linux/xarray.h>
#include <linux/jiffies.h>
#include <linux/mm_types.h>
#include <linux/pagemap.h>
#include <linux/ratelimit.h>
#include <linux/shrinker.h>
#include <linux/mmu_notifier.h>
#include <linux/delay.h>
#include <linux/refcount.h>
#include <swmc/page_coherence.h>
#include <swmc/page_replication_info.h>
#include <linux/mmdebug.h>



// TODO: delete debug
int dump_count = 0;

#define ENTIRELY_MAPPED		0x800000
#define FOLIO_PAGES_MAPPED	(ENTIRELY_MAPPED - 1)
static inline int folio_nr_pages_mapped(const struct folio *folio)
{
	return atomic_read(&folio->_nr_pages_mapped) & FOLIO_PAGES_MAPPED;
}


struct replica_page_meta {
    struct folio *folio;
    unsigned int order;
    struct list_head lru;
    refcount_t refcount;
    bool on_lru;
};

static LIST_HEAD(replica_active_lru);
static LIST_HEAD(replica_inactive_lru);
static DEFINE_SPINLOCK(replica_lru_lock);
static DEFINE_XARRAY(replica_meta_xa);

/* XArrays for page mapping */
static DEFINE_XARRAY(original_to_replica_xa);
static DEFINE_XARRAY(replica_to_original_xa);

/* Constants for replica management */
#define REPLICA_MAX_RETRIES             3
#define REPLICA_DEFAULT_SCAN_PAGES      128
#define REPLICA_INACTIVE_THRESHOLD_MULT 2
#define REPLICA_AGING_MULT              4
#define REPLICA_ACTIVE_TO_INACTIVE_RATIO 4  /* 1/4 of active pages count for shrinking */
#define REPLICA_MAX_LIST_COUNT          (1UL << 20)

/* Helper macros for validation */
#define REPLICA_FOLIO_IS_VALID(folio) ((folio) && !folio_test_reserved(folio))
#define REPLICA_SIZE_IS_VALID(size) ((size) == PAGE_SIZE || (size) == PMD_SIZE)

/* Error codes for replica operations */
enum replica_error {
    REPLICA_SUCCESS = 0,
    REPLICA_ERROR_NOMEM = -ENOMEM,
    REPLICA_ERROR_INVAL = -EINVAL,
    REPLICA_ERROR_EXIST = -EEXIST,
    REPLICA_ERROR_NOENT = -ENOENT,
    REPLICA_ERROR_IO = -EIO
};

static inline void mark_folio_uptodate(struct folio *folio, unsigned int order)
{
    /*
     * For compound folios, the uptodate flag is automatically 
     * handled by the folio layer.
     */
    folio_mark_uptodate(folio);
}

/* Reference counting helpers for safe lock dropping */
static void replica_meta_put_ref(struct replica_page_meta *meta)
{
    if (refcount_dec_and_test(&meta->refcount))
        kfree(meta);
}

/* Safely decrement reference count by 2 atomically - for double reference release */
static void replica_meta_put_ref_double(struct replica_page_meta *meta)
{
    /*
     * Use refcount_sub_and_test() to atomically decrement by 2.
     * This prevents race conditions where another thread could access
     * the metadata between two separate decrements.
     */
    if (refcount_sub_and_test(2, &meta->refcount))
        kfree(meta);
}

/* Enhanced folio validation that checks compound folio integrity */
static int replica_validate_compound_folio(struct folio *folio, unsigned int expected_order)
{
    unsigned int actual_order;
    
    if (!folio) {
        return REPLICA_ERROR_INVAL;
    }
    
    /* Check if this is actually a large folio when expected */
    if (expected_order > 0) {
        if (!folio_test_large(folio)) {
            pr_err("[%s] Expected large folio (order=%u) but got small folio %p\n",
                   __func__, expected_order, folio);
            return REPLICA_ERROR_INVAL;
        }
        
        actual_order = folio_order(folio);
        if (actual_order != expected_order) {
            pr_err("[%s] Order mismatch: expected=%u, actual=%u for folio %p\n",
                   __func__, expected_order, actual_order, folio);
            return REPLICA_ERROR_INVAL;
        }
        
        pr_info("[%s] Large folio %p (order=%u) structure validated successfully\n",
                __func__, folio, expected_order);
    }
    
    return REPLICA_SUCCESS;
}

/* Helper to validate folio before freeing */
static int replica_validate_folio_for_free(struct folio *folio, unsigned long *pfn_out)
{
    if (!folio) {
        return REPLICA_ERROR_INVAL;
    }
    
    /* Store PFN before any potential free operation */
    if (pfn_out) {
        *pfn_out = folio_pfn(folio);
    }
    
    /* Verify folio is safe to free */
    if (folio_ref_count(folio) > 1) {
        pr_err("[%s] Folio %p still has references (refcount=%d), cannot free\n",
               __func__, folio, folio_ref_count(folio));
        return REPLICA_ERROR_IO;
    }
    
    /* Check for any remaining mapcount (should be 0 after successful unmap) */
    if (folio_mapcount(folio) != 0) {
        pr_err("[%s] Folio %p still has mappings (mapcount=%d), cannot free\n",
               __func__, folio, folio_mapcount(folio));
        return REPLICA_ERROR_IO;
    }
    
    return REPLICA_SUCCESS;
}

/* Helper to safely free folio after validation */
static int replica_safe_free_folio(struct folio *folio, unsigned int order, const char *context)
{
    unsigned long pfn;
    int ret;
    
    pr_info("[%s] Attempting to free replica folio %p (order=%u) - context: %s\n",
            __func__, folio, order, context);
    /* Validate folio and get PFN before freeing */
    ret = replica_validate_folio_for_free(folio, &pfn);
    pr_info("[%s] Validation result for folio %p: ret=%d, pfn=0x%lx\n",
            __func__, folio, ret, pfn);
    if (ret != REPLICA_SUCCESS) {
        return ret;
    }
    
    /* Log before freeing */
    pr_info("[%s] About to free replica folio %p (order=%u, pfn=0x%lx) - context: %s\n", 
            __func__, folio, order, pfn, context);
    
    /* Safe to free now */
    folio_put(folio);
    
    /* Log after freeing (using saved PFN) */
    pr_info("[%s] Successfully freed replica folio pfn=0x%lx (order=%u) - context: %s\n", 
            __func__, pfn, order, context);
    
    return REPLICA_SUCCESS;
}

/* Helper to get meta with reference under lock */
static struct replica_page_meta *replica_get_meta_with_ref(struct folio *folio)
{
    struct replica_page_meta *meta;
    unsigned long flags;
    
    spin_lock_irqsave(&replica_lru_lock, flags);
    meta = xa_load(&replica_meta_xa, (unsigned long)folio);
    if (meta && refcount_inc_not_zero(&meta->refcount)) {
        spin_unlock_irqrestore(&replica_lru_lock, flags);
        return meta;
    }
    spin_unlock_irqrestore(&replica_lru_lock, flags);
    return NULL;
}

/* Simple and unified copy function - caller handles all mapping */
static int replica_copy_data(void *src_kaddr, void *dst_kaddr, size_t size)
{
    if (!src_kaddr || !dst_kaddr) {
        pr_err("[%s] NULL address provided: src=%p, dst=%p\n", 
               __func__, src_kaddr, dst_kaddr);
        return REPLICA_ERROR_INVAL;
    }
    
    memcpy(dst_kaddr, src_kaddr, size);
    pr_info("[%s] Copied %zu bytes from %p to %p\n", 
            __func__, size, src_kaddr, dst_kaddr);
    
    return REPLICA_SUCCESS;
}

/* Helper function to safely map folios using appropriate mapping method */
static void *replica_map_folio_safe(struct folio *folio, size_t size, const char *operation)
{
    unsigned int order = folio_order(folio);
    bool in_atomic = in_atomic() || irqs_disabled();
    
    if (order > 0) {
        /* For large folios */
        if (in_atomic) {
            /* In atomic context - use kmap_atomic which is safe but limited */
            void *kaddr = kmap_atomic(&folio->page);
            if (!kaddr) {
                pr_err("[%s] Failed to kmap_atomic large folio for %s\n", __func__, operation);
                return NULL;
            }

            pr_info("[%s] Mapped large folio (order=%u) using kmap_atomic for %s (atomic context), nr_pages_mapped=%u\n",
                    __func__, order, operation, folio_nr_pages_mapped(folio));
            return kaddr;
        } else {
            /* Not in atomic context - use blocking kmap */
            void *kaddr = kmap(&folio->page);
            if (!kaddr) {
                pr_err("[%s] Failed to kmap large folio for %s\n", __func__, operation);
                return NULL;
            }

            pr_info("[%s] Mapped large folio (order=%u) using kmap for %s (sleepable context), nr_pages_mapped=%u\n",
                    __func__, order, operation, folio_nr_pages_mapped(folio));
            return kaddr;
        }
    } else {
        /* Single page folio - always use kmap_local_page (safe in all contexts) */
        void *kaddr = kmap_local_page(&folio->page);
        if (!kaddr) {
            pr_err("[%s] Failed to kmap single page folio for %s\n", __func__, operation);
        }
        pr_info("[%s] Mapped single page folio using kmap_local_page for %s (order=0), nr_pages_mapped=%u\n", __func__, operation, folio_nr_pages_mapped(folio));
        return kaddr;
    }
}

/* Helper function to safely unmap folios */
static void replica_unmap_folio_safe(void *kaddr, struct folio *folio, size_t size)
{
    unsigned int order = folio_order(folio);
    bool in_atomic = in_atomic() || irqs_disabled();
    
    if (order > 0) {
        /* For large folios */
        if (in_atomic) {
            kunmap_atomic(kaddr);  /* Use kunmap_atomic for atomic context */
            pr_info("[%s] Unmapped large folio (order=%u) using kunmap_atomic, nr_pages_mapped=%u\n", __func__, order, folio_nr_pages_mapped(folio));
        } else {
            kunmap(&folio->page);  /* Use kunmap for sleepable context */
            pr_info("[%s] Unmapped large folio (order=%u) using kunmap, nr_pages_mapped=%u\n", __func__, order, folio_nr_pages_mapped(folio));
        }
    } else {
        kunmap_local(kaddr);  /* Use kunmap_local for single page folios */
        pr_info("[%s] Unmapped single page folio using kunmap_local, nr_pages_mapped=%u\n", __func__, folio_nr_pages_mapped(folio));
    }
}

/* Internal helpers for unified workflow */
static int replica_unmap_and_free_folio(struct folio *replica_folio, unsigned int order);

/* XArray mapping management helpers */
static unsigned long replica_get_original_pfn(struct folio *replica_folio);
static void replica_remove_mapping(struct folio *replica_folio, unsigned long pfn_key);
static void replica_restore_mapping(struct folio *replica_folio, unsigned long pfn_key);

/* Error handling helpers */
static int replica_handle_missing_lru_meta(struct folio *replica_folio, 
                                          unsigned long pfn_key, 
                                          struct folio *folio,
                                          const char *operation);

/* LRU management helpers */
static int replica_atomic_lru_remove_and_process(struct folio *replica_folio,
                                                int (*work_func)(struct folio *, unsigned int),
                                                const char *operation);


static void __replica_lru_add_active(struct replica_page_meta *m)
{
    pr_info("[%s] ADD to ACTIVE: folio=%p order=%u\n", __func__, m->folio, m->order);
    list_add(&m->lru, &replica_active_lru);
}

static void __replica_lru_move_to_active_mru(struct replica_page_meta *m)
{
    pr_info("[%s] MOVE to ACTIVE: folio=%p\n", __func__, m->folio);
    list_move(&m->lru, &replica_active_lru);
}

static void __replica_lru_move_to_inactive_mru(struct replica_page_meta *m)
{
    pr_info("[%s] MOVE to INACTIVE: folio=%p\n", __func__, m->folio);
    list_move(&m->lru, &replica_inactive_lru);
}

static void __replica_lru_del(struct replica_page_meta *m)
{
    pr_info("[%s] DEL: folio=%p\n", __func__, m->folio);
    list_del_init(&m->lru);
    m->on_lru = false;
}

static int replica_lru_insert(struct folio *folio, unsigned int order)
{
    struct replica_page_meta *m;
    void *ret;

    m = kzalloc(sizeof(*m), GFP_KERNEL);
    if (!m)
        return -ENOMEM;
    m->folio = folio;
    m->order = order;
    refcount_set(&m->refcount, 1);
    m->on_lru = true;
    INIT_LIST_HEAD(&m->lru);

    spin_lock(&replica_lru_lock);
    ret = xa_store(&replica_meta_xa, (unsigned long)folio, m, GFP_ATOMIC);
    if (xa_is_err(ret)) {
        spin_unlock(&replica_lru_lock);
        kfree(m);
        return xa_err(ret);
    }
    __replica_lru_add_active(m);
    spin_unlock(&replica_lru_lock);
    return 0;
}

static bool replica_hw_referenced_locked(struct folio *folio)
{
    unsigned long vm_flags = 0;
    int refs = folio_referenced(folio, 1, NULL, &vm_flags);
    
    pr_info("[%s] referenced=%d folio=%p\n", __func__, refs, folio);
    
    /* If folio was referenced, clear the accessed bit for next aging cycle */
    if (refs > 0) {
        folio_clear_referenced(folio);
        pr_info("[%s] Cleared accessed bit for referenced folio %p\n", __func__, folio);
        return true;
    }
    
    return false;
}

/* ========================================================================
 * XArray mapping management helpers
 * ======================================================================== */

/**
 * replica_get_original_pfn - Get original PFN from replica folio
 * @replica_folio: Replica folio to look up
 *
 * Returns: Original PFN key, or 0 if not found
 */
static unsigned long replica_get_original_pfn(struct folio *replica_folio)
{
    void *original_pfn_val = xa_load(&replica_to_original_xa, (unsigned long)replica_folio);
    return original_pfn_val ? xa_to_value(original_pfn_val) : 0;
}

/**
 * replica_remove_mapping - Remove bidirectional mapping between original and replica
 * @replica_folio: Replica folio
 * @pfn_key: Original PFN key
 */
static void replica_remove_mapping(struct folio *replica_folio, unsigned long pfn_key)
{
    xa_erase(&original_to_replica_xa, pfn_key);
    xa_erase(&replica_to_original_xa, (unsigned long)replica_folio);
}

/**
 * replica_restore_mapping - Restore bidirectional mapping (for error recovery)
 * @replica_folio: Replica folio
 * @pfn_key: Original PFN key
 */
static void replica_restore_mapping(struct folio *replica_folio, unsigned long pfn_key)
{
    xa_store(&original_to_replica_xa, pfn_key, replica_folio, GFP_ATOMIC);
    xa_store(&replica_to_original_xa, (unsigned long)replica_folio, xa_mk_value(pfn_key), GFP_ATOMIC);
}

/**
 * replica_handle_missing_lru_meta - Handle critical error when LRU metadata is missing
 * @replica_folio: Replica folio
 * @pfn_key: Original PFN key
 * @folio: Folio
 * @operation: Operation name for error message
 *
 * This handles the critical situation where mapping XArrays contain the replica
 * but LRU metadata is missing. This should NEVER happen in normal operation.
 */
static int replica_handle_missing_lru_meta(struct folio *replica_folio, 
                                          unsigned long pfn_key, 
                                          struct folio *folio,
                                          const char *operation)
{
    /* CRITICAL ERROR: This should NEVER happen in normal operation!
     * If we reach here, it indicates a severe bug in the replica management system:
     * - Memory corruption in LRU metadata XArray
     * - Race condition bypassing our atomic operations
     * - Logic error in replica lifecycle management
     * We restore the mapping to prevent system crash, but this needs investigation.
     */
    replica_restore_mapping(replica_folio, pfn_key);

    pr_err("[%s] CRITICAL BUG: Replica folio %p (pfn=0x%lx, folio=%p) exists in mapping XArrays but NOT in LRU metadata during %s!\n",
           __func__, replica_folio, folio_pfn(replica_folio), folio, operation);
    pr_err("[%s] This indicates a serious consistency violation - system integrity compromised!\n", __func__);
    pr_err("[%s] Mapping restored to prevent crash, but immediate investigation required!\n", __func__);

    /* Dump additional debug info */
    pr_err("[%s] Debug: original_pfn_key=%lu, replica_to_original_mapping=%p\n",
           __func__, pfn_key, xa_load(&replica_to_original_xa, (unsigned long)replica_folio));
    
    return REPLICA_ERROR_NOENT;
}

/**
 * replica_atomic_lru_remove_and_process - Atomically remove from all XArrays and process
 * @replica_folio: Replica folio to process
 * @work_func: Work function to execute (unmap_and_free or flush)
 * @operation: Operation name for error messages
 *
 * This function handles the common pattern of:
 * 1. Get original PFN mapping
 * 2. Remove from mapping XArrays 
 * 3. Remove from LRU management
 * 4. Execute work function
 * 5. Handle missing LRU metadata error
 *
 * CONCURRENCY SAFETY:
 * - Mapping XArrays are cleared while holding replica_lru_lock, preventing new lookups
 * - LRU metadata is removed atomically while holding the lock
 * - Work function executes on a folio that's no longer discoverable via XArrays
 * - Reference counting prevents premature metadata cleanup during work function execution
 * - If work function fails, all state is restored atomically
 */
static int replica_atomic_lru_remove_and_process(struct folio *replica_folio,
                                                int (*work_func)(struct folio *, unsigned int),
                                                const char *operation)
{
    struct replica_page_meta *meta;
    unsigned long flags;
    unsigned int order = 0;
    int ret;
    unsigned long pfn_key;

    if (!replica_folio) {
        pr_info("[%s] Invalid replica folio for %s\n", __func__, operation);
        return REPLICA_ERROR_INVAL;
    }

    /* Remove from both LRU and mapping XArrays atomically */
    spin_lock_irqsave(&replica_lru_lock, flags);
    
    /* Get original PFN mapping first */
    pfn_key = replica_get_original_pfn(replica_folio);
    if (!pfn_key) {
        spin_unlock_irqrestore(&replica_lru_lock, flags);
        pr_info("[%s] No original PFN mapping found for %s\n", __func__, operation);
        return REPLICA_ERROR_NOENT;
    }

    /* Remove from mapping XArrays FIRST to block new lookups */
    replica_remove_mapping(replica_folio, pfn_key);

    /* Remove from LRU management */
    meta = xa_load(&replica_meta_xa, (unsigned long)replica_folio);
    if (meta) {
        order = meta->order;
        /* Get our own reference before removing from LRU */
        refcount_inc(&meta->refcount);
        __replica_lru_del(meta);
        xa_erase(&replica_meta_xa, (unsigned long)replica_folio);
        /* Release LRU ownership reference */
        refcount_dec(&meta->refcount);
        spin_unlock_irqrestore(&replica_lru_lock, flags);
        
        /* 
         * Execute work function outside the lock.
         * SAFETY: This is safe because:
         * 1. Folio is no longer discoverable via XArrays (removed above)
         * 2. We hold a reference on metadata preventing premature cleanup
         * 3. Work functions only operate on the folio itself, not shared structures
         * 4. If work function fails, we restore state atomically below
         */
        ret = work_func(replica_folio, order);

        if (ret < 0) {
            pr_err("[%s] WORK FUNCTION FAILURE: operation=%s, error=%d\n", 
                   __func__, operation, ret);
            pr_err("[%s] Folio details: folio=%p, pfn=0x%lx, order=%u\n",
                   __func__, replica_folio, folio_pfn(replica_folio), order);
            pr_err("[%s] Folio state: refcount=%d, mapcount=%d, dirty=%d, locked=%d\n",
                   __func__, folio_ref_count(replica_folio), folio_mapcount(replica_folio),
                   folio_test_dirty(replica_folio), folio_test_locked(replica_folio));
            pr_err("[%s] Original PFN: %lu\n", __func__, pfn_key);
            
            /* 
             * Restore mapping atomically if work function failed.
             * This ensures the folio returns to a consistent state where it can
             * be found via XArrays and properly managed by LRU again.
             */
            spin_lock_irqsave(&replica_lru_lock, flags);

            replica_restore_mapping(replica_folio, pfn_key);
            xa_store(&replica_meta_xa, (unsigned long)replica_folio, meta, GFP_ATOMIC);
            __replica_lru_add_active(meta);

            spin_unlock_irqrestore(&replica_lru_lock, flags);
            
            pr_info("[%s] Restored folio %p to active LRU after %s failure\n",
                   __func__, replica_folio, operation);
        }
        
        /* Release our reference (will free meta if we're the last) */
        replica_meta_put_ref(meta);
        return ret;
    } else {
        spin_unlock_irqrestore(&replica_lru_lock, flags);
        return replica_handle_missing_lru_meta(replica_folio, pfn_key, replica_folio, operation);
    }
}

/* ========================================================================
 * Internal helpers for unified workflow
 * ======================================================================== */

/* Internal helper - XArray removal + mapping unmap + folio free */
// static int replica_unmap_and_free_folio(struct folio *replica_folio, unsigned int order)
// {
//     enum ttu_flags flags = TTU_IGNORE_MLOCK | TTU_BATCH_FLUSH;

//     if (!replica_folio) {
//         return REPLICA_ERROR_INVAL;
//     }

//     /* Note: XArray mappings should already be removed by caller */
//     struct page *temp_page = &replica_folio->page;
//     if (dump_count < 100) {
//         dump_count++;
//         dump_page(temp_page, "Replica folio before free");
//     }

//     // /* Linux vmscan.c style unmap for mapped folios */
//     // if (folio_mapped(replica_folio)) {
//     //     /*
//     //     * For large folios, use TTU_SYNC to ensure atomic processing
//     //     * and prevent partial unmap races. We don't need TTU_SPLIT_HUGE_PMD
//     //     * because we're processing the entire replica, not individual subpages.
//     //     */
//     //     if (folio_test_large(replica_folio))
//     //     flags |= TTU_SYNC;
    
//     //     if (!folio_trylock(replica_folio)) {
//     //         pr_err("[%s] Failed to lock folio %p for unmap, mapcount=%d\n",
//     //             __func__, replica_folio, folio_mapcount(replica_folio));
//     //         return REPLICA_ERROR_IO;
//     //     }
        
//     //     pr_info("[REPLICA_UNMAP] Attempting to unmap folio pfn=0x%lx (mapcount=%d)\n",
//     //             folio_pfn(replica_folio), folio_mapcount(replica_folio));
//     //     try_to_unmap(replica_folio, flags);
//     //     pr_info("[REPLICA_UNMAP] Unmap result for folio pfn=0x%lx (mapcount=%d)\n",
//     //             folio_pfn(replica_folio), folio_mapcount(replica_folio));
//     //     folio_unlock(replica_folio);
        
//     //     /* Check if still mapped after unmap attempt */
//     //     if (folio_mapped(replica_folio)) {
//     //         pr_err("[%s] Failed to unmap folio %p, mapcount=%d\n",
//     //                __func__, replica_folio, folio_mapcount(replica_folio));
//     //         return REPLICA_ERROR_IO;
//     //     }
//     // }
//     /* Linux vmscan.c style unmap for mapped folios */
//     if (folio_test_large(replica_folio))
//     flags |= TTU_SYNC;

//     if (!folio_trylock(replica_folio)) {
//         pr_err("[%s] Failed to lock folio %p for unmap, mapcount=%d\n",
//             __func__, replica_folio, folio_mapcount(replica_folio));
//         return REPLICA_ERROR_IO;
//     }
    
//     pr_info("[REPLICA_UNMAP] Attempting to unmap folio pfn=0x%lx (mapcount=%d)\n",
//             folio_pfn(replica_folio), folio_mapcount(replica_folio));
//     try_to_unmap(replica_folio, flags);
//     pr_info("[REPLICA_UNMAP] Unmap result for folio pfn=0x%lx (mapcount=%d)\n",
//             folio_pfn(replica_folio), folio_mapcount(replica_folio));
//     folio_unlock(replica_folio);
    
//     pr_info("[%s] Unmapped replica folio %p (pfn=0x%lx, order=%u)\n",
//             __func__, replica_folio, folio_pfn(replica_folio), order);
//     /* Check if still mapped after unmap attempt */
//     if (folio_mapped(replica_folio)) {
//         pr_err("[%s] Failed to unmap folio %p, mapcount=%d\n",
//                __func__, replica_folio, folio_mapcount(replica_folio));
//         return REPLICA_ERROR_IO;
//     }

//     pr_info("[%s] Folio %p successfully unmapped, proceeding to free\n",
//             __func__, replica_folio);
//     struct address_space *mapping = replica_folio->mapping;
//     filemap_invalidate_lock(mapping);
//     pr_info("[%s] Invalidate lock acquired for mapping %p\n", __func__, mapping);
//     unmap_mapping_range(mapping, replica_folio->index, folio_nr_pages(replica_folio) << PAGE_SHIFT, 1);
//     pr_info("[%s] Unmapped range for folio %p (index=%lu, nr_pages=%u)\n",
//             __func__, replica_folio, replica_folio->index, folio_nr_pages(replica_folio));
//     filemap_invalidate_unlock(mapping);
//     pr_info("[%s] Invalidate lock released for mapping %p\n", __func__, mapping);
    
//     /* Use unified validation and free function */
//     return replica_safe_free_folio(replica_folio, order, "unmap_and_free");
// }

// 디버깅 콜백 함수
static bool show_vma_cb(struct folio *folio, struct vm_area_struct *vma, unsigned long addr, void *arg)
{
    // 이 folio를 매핑하고 있는 프로세스와 VMA 정보를 출력
    pr_info("RMAP_WALK: folio pfn=0x%lx is mapped by pid=%d in vma [0x%lx - 0x%lx]\n",
            folio_pfn(folio),
            vma->vm_mm->owner->pid, // mm_struct의 owner 프로세스 ID
            vma->vm_start,
            vma->vm_end);
    return true; // 계속해서 다른 매핑도 찾기
}

void test_which_rmap_walk(struct folio *folio, struct rmap_walk_control *rwc)
{
	if (unlikely(folio_test_ksm(folio)))
		pr_info("[%s] MY_DEBUG: rmap_walk_ksm will be called.\n", __func__);
	else if (folio_test_anon(folio))
        pr_info("[%s] MY_DEBUG: rmap_walk_anon will be called.\n", __func__);
    else
        pr_info("[%s] MY_DEBUG: rmap_walk_file will be called.\n", __func__);
}

static int replica_unmap_and_free_folio(struct folio *replica_folio, unsigned int order)
{
    enum ttu_flags flags = TTU_IGNORE_MLOCK | TTU_BATCH_FLUSH;

    pr_info("[%s] Attempting to unmap and free replica folio %p (order=%u)\n",
            __func__, replica_folio, order);

    if (!replica_folio) {
        return REPLICA_ERROR_INVAL;
    }

    /* Note: XArray mappings should already be removed by caller */

    /* Linux vmscan.c style unmap for mapped folios */
    if (folio_mapped(replica_folio)) {
        pr_info("[%s] Folio %p is mapped, attempting to unmap (mapcount=%d)\n",
                __func__, replica_folio, folio_mapcount(replica_folio));
        /*
         * For large folios, use TTU_SYNC to ensure atomic processing
         * and prevent partial unmap races. We don't need TTU_SPLIT_HUGE_PMD
         * because we're processing the entire replica, not individual subpages.
         */
        if (folio_test_large(replica_folio)){
            pr_info("[%s] Large folio %p detected, setting TTU_SYNC flag\n",
                    __func__, replica_folio);
            flags |= TTU_SYNC;
        }

        pr_info("[%s] Attempting to lock folio %p for unmap (mapcount=%d)\n",
                __func__, replica_folio, folio_mapcount(replica_folio));
        folio_lock(replica_folio);
        pr_info("[%s] Folio %p locked successfully for unmap\n",
                __func__, replica_folio);
        
        // 1. rmap_walk_control 구조체 선언 및 초기화
        struct rmap_walk_control rwc = {
            .rmap_one = show_vma_cb, // 우리가 실행할 콜백 함수 지정
            .arg = NULL,             // 콜백 함수에 전달할 인자 (여기선 필요 없음)
        };
        test_which_rmap_walk(replica_folio, &rwc);

        // 2. 수정된 rmap_walk 호출 (구조체의 주소를 전달)
        rmap_walk(replica_folio, &rwc);
        pr_info("[%s] rmap_walk for replica folio %p completed\n",
                __func__, replica_folio);

        try_to_unmap(replica_folio, flags);
        pr_info("[%s] Unmap result for folio %p (mapcount=%d)\n",
                __func__, replica_folio, folio_mapcount(replica_folio));
        folio_unlock(replica_folio);
        pr_info("[%s] Unmapped replica folio %p (pfn=0x%lx, order=%u)\n",
                __func__, replica_folio, folio_pfn(replica_folio), order);
        
        /* Check if still mapped after unmap attempt */
        if (folio_mapped(replica_folio)) {
            pr_err("[%s] Failed to unmap page %p, mapcount=%d\n",
                   __func__, replica_folio, folio_mapcount(replica_folio));
            return REPLICA_ERROR_IO;
        }
    }
    
    pr_info("[%s] Folio %p successfully unmapped, proceeding to free\n",
            __func__, replica_folio);
    /* Use unified validation and free function */
    return replica_safe_free_folio(replica_folio, order, "unmap_and_free");
}

/* ========================================================================
 * Linux-style LRU implementation 
 * ======================================================================== */

/**
 * replica_reclaim_from_inactive - Reclaim pages from inactive list (Linux vmscan style)
 * @nr: Number of pages to attempt to reclaim
 *
 * This function implements Linux-style reclaim from inactive list:
 * - Takes pages from TAIL (LRU) of inactive list
 * - Referenced pages get moved back to active list MRU
 * - Non-referenced pages get unmapped and freed
 */
static unsigned long replica_reclaim_from_inactive(unsigned long nr)
{
    unsigned long flags;
    unsigned long collected = 0, freed = 0;
    struct replica_page_meta *m, *tmp;
    struct list_head process_list;
    
    INIT_LIST_HEAD(&process_list);
    
    /* First pass: collect pages from tail (LRU) of inactive list */
    spin_lock_irqsave(&replica_lru_lock, flags);
    list_for_each_entry_safe_reverse(m, tmp, &replica_inactive_lru, lru) {
        if (collected >= nr)
            break;
        
        /* Get reference and move to processing list */
        if (refcount_inc_not_zero(&m->refcount)) {
            list_move(&m->lru, &process_list);
            m->on_lru = false;
            collected++;
        }
    }
    spin_unlock_irqrestore(&replica_lru_lock, flags);
    
    pr_info("[%s] Collected %lu pages from inactive list for reclaim\n", 
            __func__, collected);
    
    /* Second pass: process folios - check references and reclaim */
    list_for_each_entry_safe(m, tmp, &process_list, lru) {
        struct folio *folio = m->folio;
        bool refd, is_dirty;
        int ret;
        unsigned long pfn_key;
        
        /* Check if referenced (last chance for inactive folios) */
        if (!folio_trylock(folio)) {
            /* Cannot lock folio - skip to avoid deadlock */
            spin_lock_irqsave(&replica_lru_lock, flags);
            __replica_lru_move_to_active_mru(m);
            m->on_lru = true;
            spin_unlock_irqrestore(&replica_lru_lock, flags);
            replica_meta_put_ref(m);
            pr_info("[%s] Skipped folio %p (could not lock) - moved to active\n", 
                    __func__, m->folio);
            continue;
        }
        
        refd = replica_hw_referenced_locked(folio);
        is_dirty = folio_test_dirty(folio);
        folio_unlock(folio);
        
        if (refd) {
            /* Referenced - promote back to active list MRU */
            spin_lock_irqsave(&replica_lru_lock, flags);
            __replica_lru_move_to_active_mru(m);
            m->on_lru = true;
            spin_unlock_irqrestore(&replica_lru_lock, flags);
            replica_meta_put_ref(m);
            pr_info("[%s] Promoted referenced folio %p back to active\n", 
                    __func__, m->folio);
            continue;
        }
        
        /* Not referenced - proceed with reclaim */
        spin_lock_irqsave(&replica_lru_lock, flags);
        
        /* Get and remove mapping */
        pfn_key = replica_get_original_pfn(m->folio);
        if (pfn_key) {
            replica_remove_mapping(m->folio, pfn_key);
        }
        
        /* Remove from LRU metadata */
        xa_erase(&replica_meta_xa, (unsigned long)m->folio);
        refcount_dec(&m->refcount); /* Release LRU ownership */
        spin_unlock_irqrestore(&replica_lru_lock, flags);
        
        /* Handle dirty vs clean folios */
        if (is_dirty) {
            /* In atomic context (shrinker), skip dirty folios to avoid kmap issues */
            if (in_atomic() || irqs_disabled()) {
                pr_info("[%s] Skipping dirty folio %p in atomic context - restored to inactive\n", 
                        __func__, m->folio);
                
                /* Restore to inactive list */
                spin_lock_irqsave(&replica_lru_lock, flags);
                replica_restore_mapping(m->folio, pfn_key);
                xa_store(&replica_meta_xa, (unsigned long)m->folio, m, GFP_ATOMIC);
                list_add_tail(&m->lru, &replica_inactive_lru);
                m->on_lru = true;
                spin_unlock_irqrestore(&replica_lru_lock, flags);
                
                replica_meta_put_ref(m);
                continue;
            }
            
            pr_info("[%s] Reclaiming dirty folio %p (pfn=0x%lx, order=%u) - writeback first\n", 
                    __func__, m->folio, folio_pfn(m->folio), m->order);
            ret = replica_folio_writeback(m->folio);
            if (ret == REPLICA_SUCCESS) {
                ret = replica_unmap_and_free_folio(m->folio, m->order);
            } else {
                pr_err("[%s] Writeback failed for dirty folio %p: %d\n", 
                       __func__, m->folio, ret);
            }
        } else {
            pr_info("[%s] Reclaiming clean folio %p (pfn=0x%lx, order=%u) - direct unmap and free\n", 
                    __func__, m->folio, folio_pfn(m->folio), m->order);
            ret = replica_unmap_and_free_folio(m->folio, m->order);
        }
        
        if (ret == REPLICA_SUCCESS) {
            freed++;
            pr_debug("[%s] Successfully reclaimed folio %p (pfn=0x%lx)\n", 
                    __func__, m->folio, folio_pfn(m->folio));
        } else {
            pr_err("[%s] RECLAIM FAILURE: folio=%p, pfn=0x%lx, order=%u, error=%d\n", 
                   __func__, m->folio, folio_pfn(m->folio), m->order, ret);
            pr_err("[%s] Folio state: refcount=%d, mapcount=%d, dirty=%d, locked=%d\n",
                   __func__, folio_ref_count(m->folio), folio_mapcount(m->folio), 
                   folio_test_dirty(m->folio), folio_test_locked(m->folio));
            pr_err("[%s] Original PFN: %lu\n", __func__, pfn_key);
            
            /* Detailed error context */
            if (ret == REPLICA_ERROR_IO) {
                if (folio_mapped(m->folio)) {
                    pr_err("[%s] Failed to unmap: folio still has %d mappings\n",
                           __func__, folio_mapcount(m->folio));
                } else if (folio_ref_count(m->folio) > 1) {
                    pr_err("[%s] Failed to free: folio still has %d references\n",
                           __func__, folio_ref_count(m->folio));
                }
            }
            
            /* Restore mapping if unmap failed */
            spin_lock_irqsave(&replica_lru_lock, flags);
            replica_restore_mapping(m->folio, pfn_key);
            xa_store(&replica_meta_xa, (unsigned long)m->folio, m, GFP_ATOMIC);
            list_add_tail(&m->lru, &replica_inactive_lru);
            m->on_lru = true;
            spin_unlock_irqrestore(&replica_lru_lock, flags);
            
            pr_info("[%s] Restored folio %p to inactive list after reclaim failure\n",
                   __func__, m->folio);
        }
        
        replica_meta_put_ref(m);
    }
    
    pr_info("[%s] Reclaimed %lu pages from inactive list\n", __func__, freed);
    return freed;
}

/**
 * replica_age_active_to_inactive - Age pages from active to inactive list
 * @nr: Number of pages to scan for aging
 *
 * Linux-style aging:
 * - Takes pages from TAIL (LRU) of active list  
 * - Referenced pages stay in active list MRU
 * - Non-referenced pages move to inactive list MRU
 */
static unsigned int replica_age_active_to_inactive(unsigned long nr)
{
    unsigned long flags;
    unsigned long collected = 0;
    unsigned int aged = 0;
    struct replica_page_meta *m, *tmp;
    struct list_head process_list;
    
    INIT_LIST_HEAD(&process_list);
    
    /* First pass: collect pages from tail (LRU) of active list */
    spin_lock_irqsave(&replica_lru_lock, flags);
    list_for_each_entry_safe_reverse(m, tmp, &replica_active_lru, lru) {
        if (collected >= nr)
            break;
        
        /* Get reference and move to processing list */
        if (refcount_inc_not_zero(&m->refcount)) {
            list_move(&m->lru, &process_list);
            m->on_lru = false;
            collected++;
        }
    }
    spin_unlock_irqrestore(&replica_lru_lock, flags);
    
    pr_info("[%s] Collected %lu pages from active list for aging\n", 
            __func__, collected);
    
    /* Second pass: check references and age appropriately */
    list_for_each_entry_safe(m, tmp, &process_list, lru) {
        struct folio *folio = m->folio;
        bool refd;
        
        /* Check hardware reference bit */
        if (!folio_trylock(folio)) {
            /* Cannot lock folio - keep in active to be safe */
            spin_lock_irqsave(&replica_lru_lock, flags);
            __replica_lru_move_to_active_mru(m);
            m->on_lru = true;
            spin_unlock_irqrestore(&replica_lru_lock, flags);
            replica_meta_put_ref(m);
            pr_info("[%s] Skipped folio %p (could not lock) - kept in active\n", 
                    __func__, m->folio);
            continue;
        }
        
        refd = replica_hw_referenced_locked(folio);
        folio_unlock(folio);
        
        spin_lock_irqsave(&replica_lru_lock, flags);
        
        if (refd) {
            /* Still referenced - keep in active list MRU */
            __replica_lru_move_to_active_mru(m);
            m->on_lru = true;
            pr_info("[%s] Keeping referenced folio %p in active\n", 
                    __func__, m->folio);
        } else {
            /* Not referenced - move to inactive list MRU */
            __replica_lru_move_to_inactive_mru(m);
            m->on_lru = true;
            aged++;
            pr_info("[%s] Aged folio %p to inactive\n", __func__, m->folio);
        }
        
        spin_unlock_irqrestore(&replica_lru_lock, flags);
        replica_meta_put_ref(m);
    }
    
    pr_info("[%s] Aged %u pages from active to inactive\n", __func__, aged);
    return aged;
}

/* ========================================================================
 * Shrinker integration (single, final)
 * ======================================================================== */

static unsigned long __replica_list_len(struct list_head *head)
{
    unsigned long n = 0;
    struct replica_page_meta *m;
    list_for_each_entry(m, head, lru) {
        if (++n > REPLICA_MAX_LIST_COUNT)
            break;
    }
    return n;
}

static unsigned long replica_shrink_count(struct shrinker *s,
                                          struct shrink_control *sc)
{
    unsigned long flags, n;
    spin_lock_irqsave(&replica_lru_lock, flags);
    n  = __replica_list_len(&replica_inactive_lru);
    n += __replica_list_len(&replica_active_lru) >> REPLICA_ACTIVE_TO_INACTIVE_RATIO;
    spin_unlock_irqrestore(&replica_lru_lock, flags);

    pr_info("[%s] shrink_count: returning %lu pages\n", __func__, n);
    return n;
}

static unsigned long replica_shrink_scan(struct shrink_control *sc)
{
    unsigned long nr_to_scan = sc->nr_to_scan ? sc->nr_to_scan : REPLICA_DEFAULT_SCAN_PAGES;
    unsigned long flags;
    unsigned long inactive_len, freed = 0;
    
    pr_info("[%s] shrink_scan: nr_to_scan=%lu\n", __func__, nr_to_scan);
    
    /* Step 1: Check if inactive list has enough pages for direct reclaim */
    spin_lock_irqsave(&replica_lru_lock, flags);
    inactive_len = __replica_list_len(&replica_inactive_lru);
    spin_unlock_irqrestore(&replica_lru_lock, flags);
    
    if (inactive_len >= nr_to_scan * REPLICA_INACTIVE_THRESHOLD_MULT) {
        /* Step 1-1: Direct reclaim from inactive list */
        freed = replica_reclaim_from_inactive(nr_to_scan);
        pr_info("[%s] Direct reclaim: inactive_len=%lu, freed=%lu\n", 
                __func__, inactive_len, freed);
        return freed;
    }
    
    /* Step 2: Not enough inactive pages, need to age active pages first */
    pr_info("[%s] Not enough inactive pages (%lu < %lu), aging active pages\n",
            __func__, inactive_len, nr_to_scan * REPLICA_INACTIVE_THRESHOLD_MULT);
    
    unsigned int aged = replica_age_active_to_inactive(nr_to_scan * REPLICA_AGING_MULT);
    
    /* Step 3: Try reclaim again after aging */
    spin_lock_irqsave(&replica_lru_lock, flags);
    inactive_len = __replica_list_len(&replica_inactive_lru);
    spin_unlock_irqrestore(&replica_lru_lock, flags);
    
    if (inactive_len >= nr_to_scan * REPLICA_INACTIVE_THRESHOLD_MULT) {
        freed = replica_reclaim_from_inactive(nr_to_scan);
    }
    
    pr_info("[%s] Final result: aged=%u, inactive_len=%lu, freed=%lu\n",
            __func__, aged, inactive_len, freed);
    
    return freed;
}

static unsigned long replica_shrink_scan_wrapper(struct shrinker *s,
                                                struct shrink_control *sc)
{
    return replica_shrink_scan(sc);
}

static struct shrinker *replica_shrinker;

/* Manual shrinker trigger function */
static void replica_trigger_shrink(unsigned long nr_to_free)
{
    struct shrink_control sc = {
        .nr_to_scan = nr_to_free,
        .gfp_mask = GFP_KERNEL,
    };
    
    unsigned long freed = replica_shrink_scan(&sc);
    pr_info("[%s] Manual shrink: requested=%lu, freed=%lu\n",
            __func__, nr_to_free, freed);
}

static int __init replica_shrinker_init(void)
{
    replica_shrinker = shrinker_alloc(0, "replica_shrinker");
    if (!replica_shrinker) {
        pr_err("[%s] failed to allocate shrinker\n", __func__);
        return -ENOMEM;
    }

    replica_shrinker->count_objects = replica_shrink_count;
    replica_shrinker->scan_objects = replica_shrink_scan_wrapper;
    replica_shrinker->seeks = DEFAULT_SEEKS;

    // shrinker_register(replica_shrinker);
    // pr_info("[%s] shrinker registered\n", __func__);
    return 0;
}

subsys_initcall(replica_shrinker_init);


/* ========================================================================
 * Public API for page coherence integration
 * ======================================================================== */

/* Helper to validate parameters for replica page creation */
static int replica_validate_create_params(void *src_kaddr, size_t size)
{
    if (!src_kaddr) {
        pr_err("[%s] NULL source address\n", __func__);
        return REPLICA_ERROR_INVAL;
    }
    
    if (!REPLICA_SIZE_IS_VALID(size)) {
        pr_err("[%s] Invalid size: %zu (expected %lu or %lu)\n", 
               __func__, size, PAGE_SIZE, PMD_SIZE);
        return REPLICA_ERROR_INVAL;
    }
    
    return REPLICA_SUCCESS;
}

/* Helper to check if replica already exists */
static int replica_check_duplicate(unsigned long pfn_key)
{
    if (xa_load(&original_to_replica_xa, pfn_key)) {
        pr_info("[%s] Replica already exists for pfn %lu\n", __func__, pfn_key);
        return REPLICA_ERROR_EXIST;
    }
    return REPLICA_SUCCESS;
}

/* Helper to allocate folio with retry and shrinking */
static struct folio *replica_allocate_folio_with_retry(unsigned int order)
{
    struct folio *replica_folio;
    gfp_t gfp_flags = GFP_HIGHUSER_MOVABLE | __GFP_ZERO;
    int retry_count = 0;
    
    if (order > 0)
        gfp_flags |= __GFP_COMP;

retry_alloc:
    replica_folio = folio_alloc(gfp_flags, order);
    if (!replica_folio) {
        if (retry_count < REPLICA_MAX_RETRIES) {
            /* Calculate how many pages to free */
            unsigned long pages_to_free = (order == 0) ? 64 : 8;
            pages_to_free = max(32UL, pages_to_free);

            pr_info("[%s] Allocation failed (retry %d/%d), triggering manual shrink of %lu pages\n",
                    __func__, retry_count + 1, REPLICA_MAX_RETRIES, pages_to_free);

            replica_trigger_shrink(pages_to_free);
            msleep(10);  /* Brief delay for shrinking to complete */
            
            retry_count++;
            goto retry_alloc;
        }

        pr_err("[%s] Failed to allocate replica folio after %d retries (order=%u)\n",
               __func__, REPLICA_MAX_RETRIES, order);
        return NULL;
    }

    /* CRITICAL: Verify large folio structure after allocation */
    if (order > 0) {
        if (WARN_ON(!folio_test_large(replica_folio))) {
            pr_err("[%s] CRITICAL: Allocated folio %p order=%u is not large!\n",
                   __func__, replica_folio, order);
            folio_put(replica_folio);
            return NULL;
        }
        
        pr_info("[%s] Successfully allocated large folio %p order=%u with %u pages\n",
                __func__, replica_folio, order, 1U << order);
        pr_info("[%s] Folio state: refcount=%d, mapcount=%d, dirty=%d, locked=%d\n",
                __func__, folio_ref_count(replica_folio), folio_mapcount(replica_folio),
                folio_test_dirty(replica_folio), folio_test_locked(replica_folio));
    }

    if (retry_count > 0) {
        pr_info("[%s] Allocation succeeded after %d retries and manual shrinking\n", 
                __func__, retry_count);
    }
    
    return replica_folio;
}

/* Helper to establish bidirectional mapping */
static int replica_establish_mapping(struct folio *replica_folio, unsigned long pfn_key)
{
    void *xa_ret;
    int err;
    
    xa_ret = xa_store(&original_to_replica_xa, pfn_key, replica_folio, GFP_KERNEL);
    if (xa_is_err(xa_ret)) {
        err = xa_err(xa_ret);
        pr_info("[%s] Failed to store original->replica mapping: %d\n", __func__, err);
        return REPLICA_ERROR_IO;
    }
    
    xa_ret = xa_store(&replica_to_original_xa, (unsigned long)replica_folio, 
                     xa_mk_value(pfn_key), GFP_KERNEL);
    if (xa_is_err(xa_ret)) {
        err = xa_err(xa_ret);
        xa_erase(&original_to_replica_xa, pfn_key);
        pr_info("[%s] Failed to store replica->original mapping: %d\n", __func__, err);
        return REPLICA_ERROR_IO;
    }
    
    return REPLICA_SUCCESS;
}

/* Helper to remove folio from LRU during error cleanup */
static void replica_lru_remove_on_error(struct folio *folio)
{
    struct replica_page_meta *meta;
    unsigned long flags;
    
    spin_lock_irqsave(&replica_lru_lock, flags);
    meta = xa_load(&replica_meta_xa, (unsigned long)folio);
    if (meta) {
        __replica_lru_del(meta);
        xa_erase(&replica_meta_xa, (unsigned long)folio);
        pr_info("[%s] Removed folio %p from LRU during error cleanup\n", __func__, folio);
    }
    spin_unlock_irqrestore(&replica_lru_lock, flags);
    
    /* Free the metadata */
    if (meta) {
        kfree(meta);
    }
}


/**
 * replica_folio_create - Create a new replica folio
 * @order: Folio order (0 for single page, PMD_ORDER for huge page)
 * @original_pfn: Original page PFN to replicate
 * @src_kaddr: Source kernel virtual address for data copy
 * @size: Size to copy (PAGE_SIZE or PMD_SIZE)
 *
 * Creates a new replica folio, adds it to LRU management, and establishes
 * bidirectional mapping with the original page.
 *
 * Returns: Replica folio pointer on success, ERR_PTR on failure
 */
struct folio *replica_folio_create(unsigned int order, pfn_t original_pfn, 
                                  void *src_kaddr, size_t size)
{
    struct folio *replica_folio;
    struct page_replication_info *replica_info;
    unsigned long pfn_key = pfn_t_to_pfn(original_pfn);
    int err;

    /* Validate input parameters */
    err = replica_validate_create_params(src_kaddr, size);
    if (err != REPLICA_SUCCESS)
        return ERR_PTR(err);

    /* Check if replica already exists */
    err = replica_check_duplicate(pfn_key);
    if (err != REPLICA_SUCCESS)
        return ERR_PTR(err);

    /* Step 1: Allocate replica folio with retry and manual shrinking */
    replica_folio = replica_allocate_folio_with_retry(order);
    if (!replica_folio)
        return ERR_PTR(REPLICA_ERROR_NOMEM);

    
    
    /* Step 2: Copy data from source to replica using unified helper */
    void *dst_kaddr = replica_map_folio_safe(replica_folio, size, "folio_create");
    if (!dst_kaddr) {
        pr_err("[%s] Failed to map replica folio for copy\n", __func__);
        err = REPLICA_ERROR_IO;
        goto free_folio;
    }
    
    err = replica_copy_data(src_kaddr, dst_kaddr, size);
    replica_unmap_folio_safe(dst_kaddr, replica_folio, size);
    
    if (err != REPLICA_SUCCESS) {
        pr_err("[%s] Data copy failed: %d\n", __func__, err);
        goto free_folio;
    }
    
    // for DEGUG NOW why folio is corrupted
    // /* Step 3: Mark folio as up to date */
    // mark_folio_uptodate(replica_folio, order);

    /* Step 4: Add to LRU management */
    err = replica_lru_insert(replica_folio, order);
    if (err) {
        pr_err("[%s] LRU insertion failed: %d\n", __func__, err);
        goto free_folio;
    }

    /* Step 5: Establish bidirectional mapping */
    err = replica_establish_mapping(replica_folio, pfn_key);
    if (err) {
        pr_err("[%s] Mapping establishment failed: %d\n", __func__, err);
        goto remove_from_lru;
    }

    // Does not need anymore
    // /* Step 6: Setup page replication info if available */
    // replica_info = get_page_replication_info(&replica_folio->page);
    // if (replica_info) {
    //     replica_info->original_pfn = original_pfn;
    //     set_page_replication_info(&replica_folio->page, replica_info);
    // }

    pr_info("[%s] Created replica folio (order=%u, pfn=0x%lx, original_pfn=0x%lx)\n",
            __func__, order, folio_pfn(replica_folio), pfn_key);

    return replica_folio;

remove_from_lru:
    /* Step 5 failed: Remove from LRU and XArray */
    replica_lru_remove_on_error(replica_folio);
    pr_err("[%s] Removed folio from LRU due to mapping failure\n", __func__);
    /* Fall through to free_folio */

free_folio:
    struct page *temp_page = &replica_folio->page;
    if (dump_count < 100) {
        dump_count++;
        dump_page(temp_page, "Replica folio before free");
    }
    folio_put(replica_folio);
    return ERR_PTR(err);
}
EXPORT_SYMBOL(replica_folio_create);

/**
 * replica_folio_find - Find existing replica folio by original PFN
 * @original_pfn: Original page PFN to look up
 *
 * Returns: Replica folio pointer if found, NULL if not found
 * 
 * Note: Caller must call replica_folio_put() when done with the returned folio
 * to release the reference and avoid memory leaks.
 */
struct folio *replica_folio_find(pfn_t original_pfn)
{
    unsigned long pfn_key = pfn_t_to_pfn(original_pfn);
    struct folio *replica_folio;
    struct replica_page_meta *meta;
    
    /* First: get replica folio pointer (may become stale) */
    replica_folio = xa_load(&original_to_replica_xa, pfn_key);
    if (!replica_folio)
        return NULL;
    
    /* Second: get reference to prevent freeing using unified helper */
    meta = replica_get_meta_with_ref(replica_folio);
    if (meta) {
        pr_info("[%s] Found replica folio %p for original pfn %lu (with reference)\n", 
                __func__, replica_folio, pfn_key);
        return replica_folio;
    } else {
        /* Failed: folio was freed or being freed */
        return NULL;
    }
}
EXPORT_SYMBOL(replica_folio_find);

/**
 * replica_folio_put - Release reference obtained from replica_folio_find()
 * @replica_folio: Replica folio to release reference for
 *
 * This function must be called for every replica folio returned by
 * replica_folio_find() to release the reference and prevent memory leaks.
 */
void replica_folio_put(struct folio *replica_folio)
{
    struct replica_page_meta *meta;
    
    if (!replica_folio)
        return;
    
    /* 
     * We need to release the reference from replica_folio_find().
     * Instead of using replica_get_meta_with_ref() which adds a temporary reference
     * and then calling put_ref twice (which creates a race condition), we directly
     * access the metadata under lock and decrement only once.
     */
    unsigned long flags;
    spin_lock_irqsave(&replica_lru_lock, flags);
    meta = xa_load(&replica_meta_xa, (unsigned long)replica_folio);
    if (meta) {
        /* Decrement the reference from replica_folio_find() */
        if (refcount_dec_and_test(&meta->refcount)) {
            spin_unlock_irqrestore(&replica_lru_lock, flags);
            kfree(meta);
        } else {
            spin_unlock_irqrestore(&replica_lru_lock, flags);
        }
        
        pr_info("[%s] Released reference for replica folio %p\n", __func__, replica_folio);
    } else {
        spin_unlock_irqrestore(&replica_lru_lock, flags);
        pr_info("[%s] No metadata found for replica folio %p during put\n", __func__, replica_folio);
    }
}
EXPORT_SYMBOL(replica_folio_put);

/**
 * replica_folio_unmap_and_free - Unmap and free a replica folio
 * @replica_folio: Replica folio to be unmapped and freed
 *
 * Returns: 0 on success, negative error code on failure
 */
int replica_folio_unmap_and_free(struct folio *replica_folio)
{
    return replica_atomic_lru_remove_and_process(replica_folio, 
                                                replica_unmap_and_free_folio,
                                                "unmap_and_free");
}
EXPORT_SYMBOL(replica_folio_unmap_and_free);

/**
 * replica_folio_writeback - Write replica data back to original page and clear dirty bit
 * @replica_folio: Replica folio to writeback
 *
 * Returns: 0 on success, negative error code on failure
 */
int replica_folio_writeback(struct folio *replica_folio)
{
    struct folio *original_folio;
    unsigned int order;
    size_t copy_size;
    int ret;
    
    if (!replica_folio) {
        pr_info("[%s] Invalid replica folio for writeback\n", __func__);
        return REPLICA_ERROR_INVAL;
    }

    order = folio_order(replica_folio);
    copy_size = PAGE_SIZE << order;

    /* Validate large folio structure before writeback */
    ret = replica_validate_compound_folio(replica_folio, order);
    if (ret != REPLICA_SUCCESS) {
        pr_err("[%s] Replica folio structure validation failed: %d\n", __func__, ret);
        return ret;
    }

    /* Get original PFN from replica->original mapping */
    unsigned long pfn_key = replica_get_original_pfn(replica_folio);
    if (!pfn_key) {
        pr_info("[%s] No original PFN mapping found for replica folio %p\n", __func__, replica_folio);
        return REPLICA_ERROR_NOENT;
    }
    original_folio = pfn_folio(pfn_key);
    
    /* Validate original folio structure */
    ret = replica_validate_compound_folio(original_folio, order);
    if (ret != REPLICA_SUCCESS) {
        pr_err("[%s] Original folio structure validation failed: %d\n", __func__, ret);
        return ret;
    }
    
    /* Map both replica and original folios safely for copying */
    void *src_kaddr = replica_map_folio_safe(replica_folio, copy_size, "writeback_src");
    if (!src_kaddr) {
        pr_err("[%s] Failed to map replica folio for writeback\n", __func__);
        return REPLICA_ERROR_IO;
    }
    
    void *dst_kaddr = replica_map_folio_safe(original_folio, copy_size, "writeback_dst");
    if (!dst_kaddr) {
        pr_err("[%s] Failed to map original folio for writeback\n", __func__);
        replica_unmap_folio_safe(src_kaddr, replica_folio, copy_size);
        return REPLICA_ERROR_IO;
    }
    
    /* Copy data from replica to original using simple method */
    ret = replica_copy_data(src_kaddr, dst_kaddr, copy_size);
    
    /* Unmap both folios */
    replica_unmap_folio_safe(dst_kaddr, original_folio, copy_size);
    replica_unmap_folio_safe(src_kaddr, replica_folio, copy_size);
    
    if (ret != REPLICA_SUCCESS) {
        return ret;
    }
    
    /* Clear dirty bit after successful writeback */
    if (folio_test_dirty(replica_folio)) {
        folio_clear_dirty(replica_folio);
    }

    pr_info("[%s] Wrote back replica folio %p to original pfn %lu\n",
            __func__, replica_folio, pfn_key);

    return REPLICA_SUCCESS;
}
EXPORT_SYMBOL(replica_folio_writeback);

/* Special work function for flush operation (writeback + unmap_and_free) */
static int replica_flush_folio(struct folio *replica_folio, unsigned int order)
{
    int ret;
    
    /* Step 1: Writeback (replica -> original data copy) */
    ret = replica_folio_writeback(replica_folio);
    if (ret == REPLICA_SUCCESS) {
        /* Step 2: Unmap and free (no more XArray operations) */
        ret = replica_unmap_and_free_folio(replica_folio, order);
    }
    
    return ret;
}

/**
 * replica_folio_flush - Flush and free a replica folio (flush = writeback + unmap_and_free)
 * @replica_folio: Replica folio to flush
 *
 * Returns: 0 on success, negative error code on failure
 */
int replica_folio_flush(struct folio *replica_folio)
{
    return replica_atomic_lru_remove_and_process(replica_folio, 
                                                replica_flush_folio,
                                                "flush");
}
EXPORT_SYMBOL(replica_folio_flush);

/**
 * replica_folio_write_protect - Remove write permissions from all mappings of a replica folio
 * @replica_folio: Replica folio to write-protect
 *
 * This function removes write permissions from all PTE/PMD entries that map
 * the replica folio, effectively making it read-only. This is useful for
 * M→S (Modified to Shared) state transitions in coherence protocols.
 *
 * Returns: Number of mappings that were write-protected, negative error code on failure
 */
int replica_folio_write_protect(struct folio *replica_folio)
{
    int cleaned;
    
    if (!replica_folio) {
        pr_info("[%s] Invalid replica folio for write protection\n", __func__);
        return REPLICA_ERROR_INVAL;
    }
    
    if (!folio_mapped(replica_folio)) {
        pr_info("[%s] Replica folio %p not mapped, no protection needed\n", __func__, replica_folio);
        return 0;
    }
    
    if (!folio_trylock(replica_folio)) {
        pr_info("[%s] Could not lock replica folio %p for write protection\n", __func__, replica_folio);
        return REPLICA_ERROR_IO;
    }
    
    /* Remove write permission from all mappings using folio_mkclean */
    cleaned = folio_mkclean(replica_folio);
    
    folio_unlock(replica_folio);

    pr_info("[%s] Write-protected %d mappings for replica folio %p\n",
            __func__, cleaned, replica_folio);

    return cleaned;
}
EXPORT_SYMBOL(replica_folio_write_protect);

