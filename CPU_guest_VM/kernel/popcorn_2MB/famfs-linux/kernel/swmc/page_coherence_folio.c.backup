/*
 * Page Coherence Management for CXL Shared Memory
 *
 * This file implements page coherence functionality for managing 
 * replica pages in CXL shared memory environments.
 */

#include <linux/mm.h>
#include <linux/pgtable.h>
#include <linux/slab.h>
#include <linux/pfn_t.h>
#include <swmc/page_coherence.h>
#include <linux/highmem.h>
#include <linux/vmalloc.h>
#include <linux/iomap.h>
#include <linux/gfp.h>
#include <linux/printk.h>
#include <linux/memcontrol.h>
#include <linux/hugetlb.h>    /* pfn_pmd, pmd_mkdirty, set_pmd_at */
#include <swmc/page_replication_info.h>
#include <linux/xarray.h>
#include <linux/completion.h>
#include <linux/spinlock.h>
#include <swmc/swmc_kmsg.h>
#include "wait_station.h"
#include <linux/rmap.h>
#include <linux/pagewalk.h>
#include <linux/mm_inline.h>
#include <linux/pagemap.h>
#include <linux/fs.h>
#include <linux/atomic.h>
#include <linux/shmem_fs.h>
#include <linux/kobject.h>
#include <linux/sysfs.h>
#include <linux/pagemap.h>

#ifdef CONFIG_PAGE_COHERENCE

/*
 * Page Coherence Management for CXL Shared Memory
 *
 * This file implements page coherence functionality for managing
 * replica pages in CXL shared memory environments.
 */

// dummy base PA for CXL HDM 
static unsigned long cxl_hdm_base = 0x1e80000000; // Default value, can be set by external module

/* Page coherence fault statistics */
static atomic64_t page_coherence_fault_count = ATOMIC64_INIT(0);
static atomic64_t page_coherence_fault_read_count = ATOMIC64_INIT(0);
static atomic64_t page_coherence_fault_write_count = ATOMIC64_INIT(0);
static atomic64_t page_coherence_replica_found_count = ATOMIC64_INIT(0);
static atomic64_t page_coherence_replica_created_count = ATOMIC64_INIT(0);

/**
 * set_cxl_hdm_base - Set the CXL HDM base address
 * @base_addr: CXL HDM base physical address
 *
 * This function allows external modules to set the CXL HDM base address
 * during their initialization phase.
 */
void set_cxl_hdm_base(unsigned long base_addr)
{
    cxl_hdm_base = base_addr;
    pr_info("[page_coherence] CXL HDM base address set to 0x%lx\n", base_addr);
}
EXPORT_SYMBOL(set_cxl_hdm_base);

/**
 * get_cxl_hdm_base - Get the current CXL HDM base address
 *
 * Returns: Current CXL HDM base address
 */
unsigned long get_cxl_hdm_base(void)
{
    return cxl_hdm_base;
}
EXPORT_SYMBOL(get_cxl_hdm_base);

/* =============================================================================
 * SYSFS INTERFACE FOR PAGE COHERENCE FAULT STATISTICS
 * ============================================================================= */

/* Sysfs show functions for fault statistics */
static ssize_t fault_count_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)
{
    return sprintf(buf, "%llu\n", atomic64_read(&page_coherence_fault_count));
}

static ssize_t fault_read_count_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)
{
    return sprintf(buf, "%llu\n", atomic64_read(&page_coherence_fault_read_count));
}

static ssize_t fault_write_count_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)
{
    return sprintf(buf, "%llu\n", atomic64_read(&page_coherence_fault_write_count));
}

static ssize_t replica_found_count_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)
{
    return sprintf(buf, "%llu\n", atomic64_read(&page_coherence_replica_found_count));
}

static ssize_t replica_created_count_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)
{
    return sprintf(buf, "%llu\n", atomic64_read(&page_coherence_replica_created_count));
}

/* Sysfs store function for resetting counters */
static ssize_t reset_counters_store(struct kobject *kobj, struct kobj_attribute *attr,
                                  const char *buf, size_t count)
{
    int reset_value;
    
    if (kstrtoint(buf, 10, &reset_value) < 0)
        return -EINVAL;
    
    if (reset_value == 1) {
        atomic64_set(&page_coherence_fault_count, 0);
        atomic64_set(&page_coherence_fault_read_count, 0);
        atomic64_set(&page_coherence_fault_write_count, 0);
        atomic64_set(&page_coherence_replica_found_count, 0);
        atomic64_set(&page_coherence_replica_created_count, 0);
        pr_info("[page_coherence] All fault counters reset\n");
    }
    
    return count;
}

/* Define sysfs attributes */
static struct kobj_attribute fault_count_attr = __ATTR_RO(fault_count);
static struct kobj_attribute fault_read_count_attr = __ATTR_RO(fault_read_count);
static struct kobj_attribute fault_write_count_attr = __ATTR_RO(fault_write_count);
static struct kobj_attribute replica_found_count_attr = __ATTR_RO(replica_found_count);
static struct kobj_attribute replica_created_count_attr = __ATTR_RO(replica_created_count);
static struct kobj_attribute reset_counters_attr = __ATTR_WO(reset_counters);

/* Array of attributes for the attribute group */
static struct attribute *page_coherence_attrs[] = {
    &fault_count_attr.attr,
    &fault_read_count_attr.attr,
    &fault_write_count_attr.attr,
    &replica_found_count_attr.attr,
    &replica_created_count_attr.attr,
    &reset_counters_attr.attr,
    NULL,
};

static struct attribute_group page_coherence_attr_group = {
    .name = "page_coherence",
    .attrs = page_coherence_attrs,
};

static struct kobject *page_coherence_kobj;

/* =============================================================================
 * MESSAGE HANDLING FUNCTIONS
 * ============================================================================= */

// Fetch message handling
// M-> S, S -> S, I -> I
static int swmc_kmsg_handle_fetch(struct swmc_kmsg_message *msg)
{
    int ret = 0;
    struct payload_data *payload = &msg->payload;
    // Validate message
    if (!msg || msg->header.type != SWMC_KMSG_TYPE_FETCH) {
        pr_err("[page_coherence] Invalid fetch message\n");
        return -EINVAL;
    }

    pr_info("[page_coherence] Handling fetch message for offset 0x%lx\n", payload->cxl_hdm_offset);

    // calculate original pfn from payload->cxl_hdm_offset
    unsigned long original_phys_addr = cxl_hdm_base + payload->cxl_hdm_offset;
    pfn_t original_pfn;

    if (payload->page_order == 0) {
        original_pfn = pfn_to_pfn_t(original_phys_addr >> PAGE_SHIFT);
    }
    else if (payload->page_order == PMD_ORDER) {
        original_pfn = pfn_to_pfn_t(original_phys_addr >> PMD_SHIFT);
    } else {
        pr_err("[page_coherence] Invalid page order: %d\n", payload->page_order);
        return -EINVAL;
    }

    // find replica folio from original pfn
    struct folio *replica_folio = replica_folio_find(original_pfn);

    if (!replica_folio) { // I -> I
        pr_info("[page_coherence] No replica folio found (I->I transition)\n");
    } else { // M -> S, S -> S
        pr_info("[page_coherence] Replica folio found (M->S or S->S transition)\n");
        
        /* For M->S transition: remove write permissions from all mappings
         * For S->S transition: already read-only, but ensure consistency
         */
        int protected_mappings = replica_folio_write_protect(replica_folio);
        if (protected_mappings > 0) {
            pr_info("[page_coherence] M->S transition: write-protected %d mappings\n", protected_mappings);
        } else if (protected_mappings == 0) {
            pr_info("[page_coherence] S->S transition: folio already read-only\n");
        } else {
            pr_info("[page_coherence] Failed to write-protect replica folio: %d\n", protected_mappings);
        }
        
        /* Release reference obtained from replica_folio_find() */
        replica_folio_put(replica_folio);
    }
    // send back fetch_ack_message using payload of original kmsg
    ret = swmc_kmsg_unicast(SWMC_KMSG_TYPE_FETCH_ACK, msg->header.ws_id, msg->header.from_nid, payload);

    return 0;
}

// Fetch_ack message handling
static int swmc_kmsg_handle_fetch_ack(struct swmc_kmsg_message *msg)
{
    // check if the message is valid
    if (!msg || msg->header.type != SWMC_KMSG_TYPE_FETCH_ACK) {
        pr_err("[page_coherence] Invalid fetch_ack message\n");
        return -EINVAL;
    }

    pr_info("[page_coherence] Handling fetch_ack message for offset 0x%lx\n", msg->payload.cxl_hdm_offset);

    // find the wait station by ID
    struct wait_station *ws = wait_station(msg->header.ws_id);
    if (!ws) {
        pr_err("[page_coherence] Invalid wait station ID: %d\n", msg->header.ws_id);
        return -EINVAL;
    }

    // Decrease pending count atomically
    if (atomic_dec_and_test(&ws->pendings_count)) {
        // All fetch ACKs received, wake up the wait station
        pr_info("[page_coherence] All fetch ACKs received for wait station %d\n", msg->header.ws_id);
        complete(&ws->pendings);
    } else {
        pr_info("[page_coherence] Fetch ACK received, pending count: %d\n",
                atomic_read(&ws->pendings_count));
    }

    return 0;
}

// Invalidate message handling
// M -> I, S -> I, I -> I (M -> I is violated)
static int swmc_kmsg_handle_invalidate(struct swmc_kmsg_message *msg)
{
    // Validate message
    if (!msg || msg->header.type != SWMC_KMSG_TYPE_INVALIDATE) {
        pr_err("[page_coherence] Invalid invalidate message\n");
        return -EINVAL;
    }

    struct payload_data *payload = &msg->payload;
    int ret = 0;

    pr_info("[page_coherence] Handling invalidate message for offset 0x%lx\n", payload->cxl_hdm_offset);
    
    // calculate original pfn from payload->cxl_hdm_offset
    unsigned long original_phys_addr = cxl_hdm_base + payload->cxl_hdm_offset;
    pfn_t original_pfn;

    if (payload->page_order == 0) {
        original_pfn = pfn_to_pfn_t(original_phys_addr >> PAGE_SHIFT);
    }
    else if (payload->page_order == PMD_ORDER) {
        original_pfn = pfn_to_pfn_t(original_phys_addr >> PMD_SHIFT);
    } else {
        pr_err("[page_coherence] Invalid page order: %d\n", payload->page_order);
        return -EINVAL;
    }

    // find replica folio from original pfn
    struct folio *replica_folio = replica_folio_find(original_pfn);

    if (!replica_folio) { // I -> I
        pr_info("[page_coherence] No replica folio found (I->I transition)\n");
    }
    else { // M -> I, S -> I
        pr_info("[page_coherence] Replica folio found (M->I or S->I transition)\n");
        
        /* For M->I or S->I transitions, clean up the replica folio
         * This includes flushing dirty data back to original and freeing the replica
         * 
         * NOTE: replica_folio_flush() internally handles all reference management
         * including the reference obtained from replica_folio_find(), so we should
         * NOT call replica_folio_put() afterwards to avoid double-free bugs.
         */
        int invalidate_ret = replica_folio_flush(replica_folio);
        if (invalidate_ret) {
            pr_info("[page_coherence] Failed to flush replica folio: %d\n", invalidate_ret);
        } else {
            pr_info("[page_coherence] Successfully flushed replica folio\n");
        }
        
        /* REMOVED: replica_folio_put(replica_folio); 
         * This was causing double-free/use-after-free bugs because
         * replica_folio_flush() already handles the reference cleanup internally.
         */
    }

    // send back invalidate_ack_message using payload of original kmsg
    ret = swmc_kmsg_unicast(SWMC_KMSG_TYPE_INVALIDATE_ACK, msg->header.ws_id, msg->header.from_nid, payload);

    return 0;
}

// Invalidate_ack message handling
static int swmc_kmsg_handle_invalidate_ack(struct swmc_kmsg_message *msg)
{
    // check if the message is valid
    if (!msg || msg->header.type != SWMC_KMSG_TYPE_INVALIDATE_ACK) {
        pr_err("[page_coherence] Invalid invalidate_ack message\n");
        return -EINVAL;
    }

    pr_info("[page_coherence] Handling invalidate_ack message for offset 0x%lx\n", msg->payload.cxl_hdm_offset);

    // find the wait station by ID
    struct wait_station *ws = wait_station(msg->header.ws_id);
    if (!ws) {
        pr_err("[page_coherence] Invalid wait station ID: %d\n", msg->header.ws_id);
        return -EINVAL;
    }

    // Decrease pending count atomically
    if (atomic_dec_and_test(&ws->pendings_count)) {
        // All invalidate ACKs received, wake up the wait station
        pr_info("[page_coherence] All invalidate ACKs received for wait station %d\n", msg->header.ws_id);
        complete(&ws->pendings);
    } else {
        pr_info("[page_coherence] Invalidate ACK received, pending count: %d\n",
                atomic_read(&ws->pendings_count));
    }

    return 0;
}

// Error message handling
static int swmc_kmsg_handle_error(struct swmc_kmsg_message *msg)
{
    // Validate message
    if (!msg || msg->header.type != SWMC_KMSG_TYPE_ERROR) {
        pr_err("[page_coherence] Invalid error message\n");
        return -EINVAL;
    }

    pr_err("[page_coherence] Received error message from node %d for offset 0x%lx\n",
           msg->header.from_nid, msg->payload.cxl_hdm_offset);

    // TODO: Handle error appropriately
    return 0;
}


/* =============================================================================
 * PAGE COHERENCE FAULT HANDLING
 * ============================================================================= */
/* Helper function to handle messaging operations */
static int broadcast_message_and_wait(enum swmc_kmsg_type msg_type, 
                                     int node_count, struct payload_data *payload,
                                     unsigned long cxl_hdm_offset)
{
    struct wait_station *ws;
    int ret;

    // register wait station for this fault
    ws = get_wait_station_multiple(current, node_count - 1);
    if (!ws) {
        pr_info("[page_coherence] Failed to get wait station\n");
        return -ENOMEM;
    }
    
    // broadcast message
    ret = swmc_kmsg_broadcast(msg_type, ws->id, payload);
    if (ret) {
        pr_info("[page_coherence] Failed to send %s message: %d\n", 
               msg_type == SWMC_KMSG_TYPE_FETCH ? "fetch" : "invalidate", ret);
        // Continue anyway for now - could implement fallback
    }

    void *wait_result = wait_at_station(ws);
    if (IS_ERR(wait_result)) {
        ret = PTR_ERR(wait_result);
        pr_info("[page_coherence] Failed to wait at station: %d\n", ret);
        return ret;
    }

    pr_info("[page_coherence] Waited at station, received response for %s message\n",
            msg_type == SWMC_KMSG_TYPE_FETCH ? "fetch" : "invalidate");
    return 0;
}

/* Helper function to handle PMD reuse case */
/* TODO: Remove if not needed after testing new logic
static int handle_pmd_reuse(pmd_t pmd_val, pfn_t original_pfn, pfn_t *pfn)
{
    pfn_t replica_pfn;
    
    pr_info("[page_coherence] Skipping replication and copying for write fault on non-writable PMD.\n");
    replica_pfn.val = pmd_pfn(pmd_val) | (original_pfn.val & PFN_FLAGS_MASK);
    *pfn = replica_pfn;
    pr_info("[page_coherence] Updated pfn to existing PMD PFN: %lu\n",
            pfn_t_to_pfn(replica_pfn));
    return 0;
}
*/

/**
 * page_coherence_fault - Handle page coherence faults
 * @vmf: Fault information structure
 * @iter: IOMAP iterator
 * @size: Size of the fault (PAGE_SIZE or PMD_SIZE)
 * @kaddr: Kernel virtual address of the original page
 * @pfn: Pointer to the page frame number, will be updated to replica PFN
 *
 * Returns: 0 on success, negative error code on failure
 */
int page_coherence_fault(struct vm_fault *vmf, const struct iomap_iter *iter,
                         size_t size, void *kaddr, pfn_t *pfn, pfn_t *pfnp)
{
    struct folio *replica_folio;
    unsigned int order;
    pfn_t original_pfn = *pfn;
    pfn_t replica_pfn;
    int ret;
    bool write = iter->flags & IOMAP_WRITE;
    // bool leader; // is this fault is the leader fault of this cxl_hdm_offset?
    struct payload_data payload;
    /* TODO: Remove if not needed after testing new logic
    pmd_t *pmd;
    pmd_t pmd_val;
    */
    int node_count;
    unsigned long cxl_hdm_offset;
    bool need_invalidate = false;

    // early return if it's not a fault to famfs
    pfn_t cxl_hdm_base_pfn = pfn_to_pfn_t(cxl_hdm_base >> PAGE_SHIFT);
    if (pfn_t_to_pfn(original_pfn) < pfn_t_to_pfn(cxl_hdm_base_pfn)) {
        pr_info("[%s] Not a CXL HDM fault, skipping page coherence handling\n", __func__);
        return 0;
    }
    
    /* Increment total fault counter */
    atomic64_inc(&page_coherence_fault_count);
    
    /* Increment read/write specific counters */
    if (write) {
        atomic64_inc(&page_coherence_fault_write_count);
    } else {
        atomic64_inc(&page_coherence_fault_read_count);
    }

    if (!vmf || !iter || !kaddr || !pfn) {
        pr_err("[%s] Invalid parameters\n", __func__);
        return -EINVAL;
    }

    /* Determine page order */
    if (size == PMD_SIZE)
        order = PMD_ORDER;
    else if (size == PAGE_SIZE)
        order = 0;
    else {
        pr_err("[%s] Unsupported size: %zu\n", __func__, size);
        return -EINVAL;
    }

    pr_info("[%s] Fault at 0x%lx: size=%zu order=%u original pfn=0x%lx\n",
        __func__, vmf->address, size, order, pfn_t_to_pfn(original_pfn));

    // Get CXL HDM offset for this fault
    cxl_hdm_offset = pfn_t_to_pfn(original_pfn) * PAGE_SIZE - cxl_hdm_base;
    
    // TODO: check if this is the leader fault for this cxl_hdm_offset. for now, assume every fault is a leader
    // leader = true;

    payload.cxl_hdm_offset = cxl_hdm_offset;
    payload.page_order = order;

    node_count = swmc_kmsg_node_count();
    if (node_count <= 0) {
        pr_err("[%s] Invalid node count: %d\n", __func__, node_count);
        return -EINVAL;
    }

    /* =======================================================================
     * STEP 1: Check if replica already exists for this original PFN
     * ======================================================================= */
    
    /* First, check if a replica folio already exists for this original PFN */
    replica_folio = replica_folio_find(original_pfn);
    
    if (replica_folio) {
        /* Replica exists - handle different fault scenarios */
        atomic64_inc(&page_coherence_replica_found_count);
        pr_info("[%s] Replica folio found: %p\n", __func__, replica_folio);
        
        if (!write) {
            /* READ FAULT on existing replica:
             * Simply map the existing replica folio without any protocol messages.
             * This handles the case where multiple processes read the same page.
             */
            pr_info("[%s] Read fault on existing replica - direct mapping\n", __func__);

            goto map_pfn;

            // /* Build replica PFN with preserved flags and return */
            // replica_pfn.val = folio_pfn(replica_folio) |
            // (original_pfn.val & PFN_FLAGS_MASK);
            // *pfn = replica_pfn;
            
            // pr_info("[%s] Mapped existing replica pfn=0x%lx at 0x%lx\n",
            //         __func__, pfn_t_to_pfn(replica_pfn), vmf->address);
            
            // ret = 0;
            // replica_folio_put(replica_folio);
            // return ret;
        } else {
            /* WRITE FAULT on existing replica:
             * Need to check if replica is already writable by ANY process in the system.
             * If already writable somewhere, this is M->M (just map it writable here too).
             * If only readable, this is S->M transition (need invalidation to others).
             */
            pr_info("[%s] Write fault on existing replica\n", __func__);

            /* Check if the replica page is currently mapped as writable anywhere in the system.
             * We use page_mapped() and check if any PTE/PMD has write permissions.
             * This tells us the current coherence state:
             * - If writable mappings exist: page is in M state
             * - If only read-only mappings exist: page is in S state
             */
            bool is_currently_writable = false;

            if (folio_mapped(replica_folio)) {

                /* Check if this page is mapped writable anywhere in the system.
                 * We can use page_mkclean() return value to determine this:
                 * - If folio_mkclean() returns > 0: had writable mappings (M state)
                 * - If folio_mkclean() returns 0: no writable mappings (S state)
                 * But we don't want to actually clean it, so we need another approach.
                 */
                
                /* Alternative: Check if page is dirty, which indicates recent writes */
                if (folio_test_dirty(replica_folio)) {
                    is_currently_writable = true;
                    pr_info("[%s] Replica page is dirty - likely in M state\n", __func__);
                } else {
                    /* Page is clean - likely in S state, but could be M with clean data */
                    pr_info("[%s] Replica page is clean - likely in S state\n", __func__);
                    is_currently_writable = false;
                }
            } else {
                /* Page not mapped anywhere - shouldn't happen for existing replica */
                pr_info("[%s] Replica exists but not mapped anywhere\n", __func__);
                is_currently_writable = false;
            }
            
            if (is_currently_writable) {
                /* Replica is already in M state - just map it writable here too (M->M) */
                pr_info("[%s] Replica already writable elsewhere - M->M transition\n", __func__);

                goto map_pfn;
                
                // /* No protocol messages needed - just map the existing replica as writable */
                // replica_pfn.val = folio_pfn(replica_folio) |
                //                   (original_pfn.val & PFN_FLAGS_MASK);
                // *pfn = replica_pfn;

                // pr_info("[%s] Mapped existing writable replica pfn=0x%lx at 0x%lx\n",
                //         __func__, pfn_t_to_pfn(replica_pfn), vmf->address);
                
                // ret = 0;
                // replica_folio_put(replica_folio);
                // return ret;
            } else {
                /* Replica is in S state - need S->M transition */
                pr_info("[%s] S->M transition needed for existing replica\n", __func__);

                /* Send invalidation to other nodes for S->M transition */
                ret = broadcast_message_and_wait(SWMC_KMSG_TYPE_INVALIDATE, node_count, 
                                               &payload, cxl_hdm_offset);
                if (ret) {
                    replica_folio_put(replica_folio);
                    return ret;
                }

                goto map_pfn;
                // /* Map the existing replica as writable */
                // replica_pfn.val = folio_pfn(replica_folio) |
                //                   (original_pfn.val & PFN_FLAGS_MASK);
                // *pfn = replica_pfn;

                // pr_info("[%s] Mapped existing replica as writable pfn=0x%lx at 0x%lx\n",
                //         __func__, pfn_t_to_pfn(replica_pfn), vmf->address);
                
                // ret = 0;
                // replica_folio_put(replica_folio);
                // return ret;
            }
        }
    }
    
    /* =======================================================================
     * STEP 2: No replica exists - create new replica (I->S or I->M)
     * ======================================================================= */

    pr_info("[%s] No replica found - creating new replica\n", __func__);

    /* Determine what protocol messages are needed for new replica creation */
    if (write) {
        /* I->M transition: Need both fetch and invalidate */
        pr_info("[%s] I->M transition: need fetch + invalidate\n", __func__);
        need_invalidate = true;
    } else {
        /* I->S transition: Only need fetch */
        pr_info("[%s] I->S transition: need fetch only\n", __func__);
        need_invalidate = false;
    }
    
    /* Send fetch message to other nodes */
    ret = broadcast_message_and_wait(SWMC_KMSG_TYPE_FETCH, node_count, 
                                   &payload, cxl_hdm_offset);
    if (ret)
        return ret;
    
    /* Send invalidate message if needed (for I->M transition) */
    if (need_invalidate) {
        pr_info("[%s] Broadcasting invalidate message for I->M transition\n", __func__);
        ret = broadcast_message_and_wait(SWMC_KMSG_TYPE_INVALIDATE, node_count, 
                                       &payload, cxl_hdm_offset);
        if (ret)
            return ret;
    }

    /* Create replica folio using page_replication.c API */
    replica_folio = replica_folio_create(order, original_pfn, kaddr, size);
    if (IS_ERR(replica_folio)) {
        ret = PTR_ERR(replica_folio);
        pr_info("[%s] Failed to create replica folio: %d\n", __func__, ret);
        return ret;
    }
    
    /* Increment replica created counter */
    atomic64_inc(&page_coherence_replica_created_count);

    // map virtual address to replica folio
    pr_info("[%s] Created new replica folio: %p\n", __func__, replica_folio);

    // struct address_space *mapping = vmf->vma->vm_file->f_mapping;
    // replica_folio->mapping = mapping;
    // replica_folio->index = vmf->pgoff;

    // unsigned long num = 1UL << order;
    // struct page *pages[num];
    // // aligned to 2MiB
    // unsigned long base_address = vmf->address & ~((num * PAGE_SIZE) - 1);

    // for (unsigned long i = 0; i < num; i++) {
    //     pages[i] = folio_page(replica_folio, i);
    // }

    // ret = vm_insert_pages(vmf->vma, base_address, pages, &num);
    // if (ret) {
    //     pr_err("[%s] Failed to insert pages for replica folio: %d, num=%lu\n", __func__, ret, num);
    //     /* Clean up the replica folio if insertion failed */
    //     replica_folio_unmap_and_free(replica_folio);
    //     return ret;
    // }

map_pfn:
    // pr_info("[%s] Mapping replica folio to vaddr=0x%lx\n", __func__, vmf->address);

    // pr_info("    [%s] Adding rmap for replica folio, folio_mapped=%d\n", __func__, folio_mapped(replica_folio));
    // rmap_t rmap_flags = RMAP_NON;
    // folio_add_anon_rmap_pmd(replica_folio, folio_page(replica_folio, 0), vmf->vma, vmf->address, rmap_flags);
    // pr_info("    [%s] Added rmap for replica folio, folio_mapped=%d\n", __func__, folio_mapped(replica_folio));

    // //sanity check: ensure rmap_walk with


    // /* Build new PFN with preserved flags and update */
    // replica_pfn.val = folio_pfn(replica_folio) |
    //                   (original_pfn.val & PFN_FLAGS_MASK);

    // pr_info("[COHERENCE_MAP] PID=%d maps pfn=0x%lx to vaddr=0x%lx (write=%d)\n",
    //     current->pid, pfn_t_to_pfn(replica_pfn), vmf->address, write);
    // *pfn = replica_pfn;

    // replica_folio_put(replica_folio);

    // if (write && iter->iomap->flags & IOMAP_F_SHARED) {
	// 	err = dax_iomap_copy_around(pos, size, size, srcmap, kaddr);
	// 	if (err)
	// 		return dax_fault_return(err);
	// }

	// if (dax_fault_is_synchronous(iter, vmf->vma))
	// 	return dax_fault_synchronous_pfnp(pfnp, pfn);

	// /* insert PMD pfn */
	// if (size == PMD_SIZE) {
	// 	// sungsu: vmf_insert_pfn_pmd is called
	// 	pr_info("[dax_fault_iter] Call vmf_insert_pfn_pmd() for PMD pfn insertion.\n");
	// 	*pfnp = pfn;
		
	// 	return vmf_insert_pfn_pmd(vmf, pfn, write);
	// }

	// /* insert PTE pfn */
	// if (write) {
	// 	// sungsu: vmf_insert_mixed_mkwrite is called
	// 	pr_info("[dax_fault_iter] Call vmf_insert_mixed_mkwrite() for PTE pfn insertion.\n");
	// 	return vmf_insert_mixed_mkwrite(vmf->vma, vmf->address, pfn);
	// }
	// // insert PTE pfn
	// // sungsu: vmf_insert_mixed is called
	// pr_info("[dax_fault_iter] Call vmf_insert_mixed() for PTE pfn insertion.\n");
	// return vmf_insert_mixed(vmf->vma, vmf->address, pfn);

    // return VM_FAULT_NOPAGE;

    

//     /*
//     NEW LOGIC START: to make folio unmappable
//     */

//     struct address_space *mapping;
//     int error;

//     // (1) 폴트를 발생시킨 VMA(Virtual Memory Area)로부터 파일의 address_space를 가져온다.
//     mapping = vmf->vma->vm_file->f_mapping;
    
//     if (!vmf->vma->vm_file) {
//         // 파일 기반이 아닌 VMA (예: 익명 메모리)는 이 로직에서 처리할 수 없음.
//         // 이 경우, 익명 페이지를 위한 다른 unmap 메커니즘을 고려해야 함.
//         pr_err("[%s] Fault on non-file-backed VMA. Cannot attach folio to mapping.\n", __func__);
//         // 여기서 적절한 에러 처리 및 생성된 folio 롤백 필요
//         replica_folio_unmap_and_free(replica_folio); // 예시 롤백
//         return -EINVAL;
//     }

//     // (2) folio의 인덱스(파일 내 오프셋)를 설정한다.
//     // vmf->pgoff는 파일의 시작을 기준으로 한 페이지 오프셋이다.
//     replica_folio->index = vmf->pgoff;

//     // (3) folio를 파일의 페이지 캐시(address_space)에 명시적으로 추가한다.
//     // 이 함수가 folio->mapping = mapping 설정과 Rmap 등록을 담당한다.
//     // GFP_KERNEL은 sleep이 가능함을 의미. 이 컨텍스트에서는 괜찮다.
//     error = filemap_add_folio(mapping, replica_folio, vmf->pgoff, GFP_KERNEL);
//     if (error) {
//         pr_err("[%s] Failed to add replica folio to filemap: %d\n", __func__, error);
//         // folio_add_to_page_cache 실패 시 folio_put으로 참조 카운트 정리
//         folio_put(replica_folio);

//         // 원래 pfn으로 폴트를 재시도하도록 하거나, 에러를 반환
//         *pfn = original_pfn; // 원래 pfn으로 되돌리기 for safety
//         return error;
//     }

//     pr_info("[%s] Successfully attached replica folio pfn=%lu to mapping (inode=%lu, index=%lu)\n",
//     __func__, folio_pfn(replica_folio), mapping->host->i_ino, replica_folio->index);
    
//     // (4) 이제 folio는 페이지 캐시의 일부가 되었으므로, 잠금을 풀고 dirty로 마크할 수 있다.
//     // DAX는 페이지 캐시를 다르게 사용하므로 이 부분은 fs/dax.c의 로직을 참고해야 함.
//     // 예를 들어, 쓰기 폴트였다면 folio_mark_dirty()를 호출할 수 있다.
//     if (write) {
//         folio_mark_dirty(replica_folio);
//     }
//     folio_unlock(replica_folio); // folio_add_to_page_cache에서 lock을 잡았으므로 풀어준다.


// map_pfn:

//     replica_pfn.val = folio_pfn(replica_folio) |
//                       (original_pfn.val & PFN_FLAGS_MASK);

//     pr_info("[COHERENCE_MAP] PID=%d maps pfn=0x%lx to vaddr=0x%lx (write=%d)\n",
//         current->pid, pfn_t_to_pfn(replica_pfn), vmf->address, write);

//     folio_put(replica_folio); // folio_add_to_page_cache에서 참조 카운트가 증가했으므로, 여기서 감소시킨다.
    
//     *pfn = replica_pfn;

//     /*
//     NEW LOGIC END
//     */

    
    // Validate original folio exists for devdax
    struct folio *original_folio_ptr = pfn_folio(pfn_t_to_pfn(original_pfn));
    if (!original_folio_ptr) {
        pr_info("[%s] Invalid original folio pointer\n", __func__);
        /* Clean up the replica folio we just created */
        replica_folio_unmap_and_free(replica_folio);
        return -EINVAL;
    }
    pr_info("[%s] Original folio: %p, replica folio: %p\n", __func__, original_folio_ptr, replica_folio);

    /* Build new PFN with preserved flags and update */
    replica_pfn.val = folio_pfn(replica_folio) |
                      (original_pfn.val & PFN_FLAGS_MASK);

    pr_info("[COHERENCE_MAP] PID=%d maps pfn=0x%lx to vaddr=0x%lx (write=%d)\n",
        current->pid, pfn_t_to_pfn(replica_pfn), vmf->address, write);
    *pfn = replica_pfn;

    pr_info("[%s] Replicated pfn=0x%lx at 0x%lx\n",
            __func__, pfn_t_to_pfn(replica_pfn), vmf->address);
    return 0;
}

/**
 * page_coherence_init - Initialize page coherence subsystem
 *
 * Returns: 0 on success, negative error code on failure
 */
int __init page_coherence_init(void)
{
    int ret;
    
    pr_info("[page_coherence] Initializing page coherence subsystem\n");

    // Register message handlers
    ret = swmc_kmsg_register_callback(SWMC_KMSG_TYPE_FETCH, swmc_kmsg_handle_fetch);
    if (ret) {
        pr_info("[page_coherence] Failed to register fetch handler: %d\n", ret);
        return ret;
    }

    ret = swmc_kmsg_register_callback(SWMC_KMSG_TYPE_INVALIDATE, swmc_kmsg_handle_invalidate);
    if (ret) {
        pr_info("[page_coherence] Failed to register invalidate handler: %d\n", ret);
        return ret;
    }

    ret = swmc_kmsg_register_callback(SWMC_KMSG_TYPE_FETCH_ACK, swmc_kmsg_handle_fetch_ack);
    if (ret) {
        pr_info("[page_coherence] Failed to register fetch_ack handler: %d\n", ret);
        return ret;
    }

    ret = swmc_kmsg_register_callback(SWMC_KMSG_TYPE_INVALIDATE_ACK, swmc_kmsg_handle_invalidate_ack);
    if (ret) {
        pr_info("[page_coherence] Failed to register invalidate_ack handler: %d\n", ret);
        return ret;
    }

    ret = swmc_kmsg_register_callback(SWMC_KMSG_TYPE_ERROR, swmc_kmsg_handle_error);
    if (ret) {
        pr_info("[page_coherence] Failed to register error handler: %d\n", ret);
        return ret;
    }

    /* Create sysfs interface for fault statistics */
    page_coherence_kobj = kobject_create_and_add("swmc", kernel_kobj);
    if (!page_coherence_kobj) {
        pr_err("[page_coherence] Failed to create kobject\n");
        return -ENOMEM;
    }

    ret = sysfs_create_group(page_coherence_kobj, &page_coherence_attr_group);
    if (ret) {
        pr_err("[page_coherence] Failed to create sysfs group: %d\n", ret);
        kobject_put(page_coherence_kobj);
        return ret;
    }

    pr_info("[page_coherence] Page coherence subsystem initialized successfully\n");
    pr_info("[page_coherence] Sysfs interface available at /sys/kernel/swmc/page_coherence/\n");
    return 0;
}

subsys_initcall(page_coherence_init);

#endif /* CONFIG_PAGE_COHERENCE */
