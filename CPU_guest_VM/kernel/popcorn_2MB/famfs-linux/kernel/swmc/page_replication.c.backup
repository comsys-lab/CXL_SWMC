// SPDX-License-Identifier: GPL-2.0
// Replica pages: LRU with **hardware accessed (young) bit** via folio_referenced(), plus pr_info debug logs only.

#include <linux/list.h>
#include <linux/rmap.h>
#include <linux/swap.h>
#include <linux/mm.h>
#include <linux/highmem.h>
#include <linux/gfp.h>
#include <linux/xarray.h>
#include <linux/jiffies.h>
#include <linux/mm_types.h>
#include <linux/pagemap.h>
#include <linux/ratelimit.h>
#include <linux/shrinker.h>
#include <linux/mmu_notifier.h>
#include <linux/delay.h>
#include <linux/refcount.h>
#include <swmc/page_coherence.h>
#include <swmc/page_replication_info.h>

struct replica_page_meta {
    struct page *head;
    unsigned int order;
    struct list_head lru;
    refcount_t refcount;
    bool on_lru;
};

static LIST_HEAD(replica_active_lru);
static LIST_HEAD(replica_inactive_lru);
static DEFINE_SPINLOCK(replica_lru_lock);
static DEFINE_XARRAY(replica_meta_xa);

/* XArrays for page mapping */
static DEFINE_XARRAY(original_to_replica_xa);
static DEFINE_XARRAY(replica_to_original_xa);

/* Constants for replica management */
#define REPLICA_MAX_RETRIES             3
#define REPLICA_DEFAULT_SCAN_PAGES      128
#define REPLICA_INACTIVE_THRESHOLD_MULT 2
#define REPLICA_AGING_MULT              4
#define REPLICA_ACTIVE_TO_INACTIVE_RATIO 4  /* 1/4 of active pages count for shrinking */
#define REPLICA_MAX_LIST_COUNT          (1UL << 20)

/* Helper macros for validation */
#define REPLICA_PAGE_IS_VALID(page) ((page) && !PageReserved(page))
#define REPLICA_SIZE_IS_VALID(size) ((size) == PAGE_SIZE || (size) == PMD_SIZE)

/* Error codes for replica operations */
enum replica_error {
    REPLICA_SUCCESS = 0,
    REPLICA_ERROR_NOMEM = -ENOMEM,
    REPLICA_ERROR_INVAL = -EINVAL,
    REPLICA_ERROR_EXIST = -EEXIST,
    REPLICA_ERROR_NOENT = -ENOENT,
    REPLICA_ERROR_IO = -EIO
};

static inline struct page *replica_head(struct page *p) { return compound_head(p); }
static inline struct folio *replica_folio(struct page *head) { return page_folio(head); }

static inline void mark_compound_uptodate(struct page *head, unsigned int order)
{
    /*
     * For compound pages, we must ONLY set flags on the head page.
     * Setting flags on tail pages corrupts their mapping fields and
     * violates kernel compound page invariants.
     * The uptodate flag is automatically inherited by tail pages.
     */
    SetPageUptodate(head);
}

/* Reference counting helpers for safe lock dropping */
static void replica_meta_put_ref(struct replica_page_meta *meta)
{
    if (refcount_dec_and_test(&meta->refcount))
        kfree(meta);
}

/* Safely decrement reference count by 2 atomically - for replica_page_put() */
static void replica_meta_put_ref_double(struct replica_page_meta *meta)
{
    /*
     * Use refcount_sub_and_test() to atomically decrement by 2.
     * This prevents race conditions where another thread could access
     * the metadata between two separate decrements.
     */
    if (refcount_sub_and_test(2, &meta->refcount))
        kfree(meta);
}

/* Enhanced page validation that checks compound page integrity */
static int replica_validate_compound_page(struct page *page, unsigned int expected_order)
{
    unsigned int actual_order;
    
    if (!page) {
        return REPLICA_ERROR_INVAL;
    }
    
    /* Check if this is actually a compound page when expected */
    if (expected_order > 0) {
        if (!PageCompound(page)) {
            pr_err("[%s] Expected compound page (order=%u) but got simple page %p\n",
                   __func__, expected_order, page);
            return REPLICA_ERROR_INVAL;
        }
        
        actual_order = compound_order(page);
        if (actual_order != expected_order) {
            pr_err("[%s] Order mismatch: expected=%u, actual=%u for page %p\n",
                   __func__, expected_order, actual_order, page);
            return REPLICA_ERROR_INVAL;
        }
        
        /* Validate tail pages are properly linked */
        unsigned int nr_pages = 1U << expected_order;
        unsigned int i;
        
        for (i = 1; i < nr_pages; i++) {
            struct page *tail = page + i;
            
            if (!PageTail(tail)) {
                pr_err("[%s] Tail page %u (page %p) not marked as tail\n",
                       __func__, i, tail);
                return REPLICA_ERROR_IO;
            }
            
            if (compound_head(tail) != page) {
                pr_err("[%s] Tail page %u (page %p) head pointer corrupted: %p != %p\n",
                       __func__, i, tail, compound_head(tail), page);
                return REPLICA_ERROR_IO;
            }
        }
        
        pr_info("[%s] Compound page %p (order=%u) structure validated successfully\n",
                __func__, page, expected_order);
    }
    
    return REPLICA_SUCCESS;
}

/* Helper to validate page before freeing */
static int replica_validate_page_for_free(struct page *replica_page, unsigned long *pfn_out)
{
    struct folio *folio;
    
    if (!replica_page) {
        return REPLICA_ERROR_INVAL;
    }
    
    folio = page_folio(replica_page);
    
    /* Store PFN before any potential free operation */
    if (pfn_out) {
        *pfn_out = page_to_pfn(replica_page);
    }
    
    /* Verify page is safe to free */
    if (folio_ref_count(folio) > 1) {
        pr_err("[%s] Page %p still has references (refcount=%d), cannot free\n",
               __func__, replica_page, folio_ref_count(folio));
        return REPLICA_ERROR_IO;
    }
    
    /* Check for any remaining mapcount (should be 0 after successful unmap) */
    if (folio_mapcount(folio) != 0) {
        pr_err("[%s] Page %p still has mappings (mapcount=%d), cannot free\n",
               __func__, replica_page, folio_mapcount(folio));
        return REPLICA_ERROR_IO;
    }
    
    return REPLICA_SUCCESS;
}

/* Helper to safely free page after validation */
static int replica_safe_free_page(struct page *replica_page, unsigned int order, const char *context)
{
    unsigned long pfn;
    int ret;
    
    /* Validate page and get PFN before freeing */
    ret = replica_validate_page_for_free(replica_page, &pfn);
    if (ret != REPLICA_SUCCESS) {
        return ret;
    }
    
    /* Log before freeing */
    pr_info("[%s] About to free replica page %p (order=%u, pfn=%lu) - context: %s\n", 
            __func__, replica_page, order, pfn, context);
    
    /* Safe to free now */
    if (order > 0)
        __free_pages(replica_page, order);
    else
        __free_page(replica_page);
    
    /* Log after freeing (using saved PFN) */
    pr_info("[%s] Successfully freed replica page pfn=%lu (order=%u) - context: %s\n", 
            __func__, pfn, order, context);
    
    return REPLICA_SUCCESS;
}

/* Helper to get meta with reference under lock */
static struct replica_page_meta *replica_get_meta_with_ref(struct page *head)
{
    struct replica_page_meta *meta;
    unsigned long flags;
    
    spin_lock_irqsave(&replica_lru_lock, flags);
    meta = xa_load(&replica_meta_xa, (unsigned long)head);
    if (meta && refcount_inc_not_zero(&meta->refcount)) {
        spin_unlock_irqrestore(&replica_lru_lock, flags);
        return meta;
    }
    spin_unlock_irqrestore(&replica_lru_lock, flags);
    return NULL;
}

/* Simple and unified copy function - caller handles all mapping */
static int replica_copy_data(void *src_kaddr, void *dst_kaddr, size_t size)
{
    if (!src_kaddr || !dst_kaddr) {
        pr_err("[%s] NULL address provided: src=%p, dst=%p\n", 
               __func__, src_kaddr, dst_kaddr);
        return REPLICA_ERROR_INVAL;
    }
    
    memcpy(dst_kaddr, src_kaddr, size);
    pr_info("[%s] Copied %zu bytes from %p to %p\n", 
            __func__, size, src_kaddr, dst_kaddr);
    
    return REPLICA_SUCCESS;
}

/* Helper function to safely map pages using kmap for compound-aware operations */
static void *replica_map_pages_safe(struct page *page, size_t size, const char *operation)
{
    unsigned int nr_pages = size >> PAGE_SHIFT;
    unsigned int order = compound_order(page);
    
    if (order > 0 && nr_pages > 1) {
        /* For compound pages, use kmap on head page only
         * This maps the entire compound page through the head page
         * CRITICAL: Never kmap individual tail pages as it corrupts their structure
         */
        void *kaddr = kmap(page);  /* Use blocking kmap for compound pages */
        if (!kaddr) {
            pr_err("[%s] Failed to kmap compound page head for %s\n", __func__, operation);
            return NULL;
        }
        
        pr_info("[%s] Mapped compound page (order=%u, %u pages) using kmap on head for %s\n", 
                __func__, order, nr_pages, operation);
        return kaddr;
    } else {
        /* Single page - use kmap_local_page for better performance */
        void *kaddr = kmap_local_page(page);
        if (!kaddr) {
            pr_err("[%s] Failed to kmap single page for %s\n", __func__, operation);
        }
        return kaddr;
    }
}

/* Helper function to safely unmap pages */
static void replica_unmap_pages_safe(void *kaddr, struct page *page, size_t size)
{
    unsigned int nr_pages = size >> PAGE_SHIFT;
    unsigned int order = compound_order(page);
    
    if (order > 0 && nr_pages > 1) {
        kunmap(page);  /* Use kunmap for compound pages */
    } else {
        kunmap_local(kaddr);  /* Use kunmap_local for single pages */
    }
}

/* Internal helpers for unified workflow */
static int replica_unmap_and_free_page(struct page *replica_page, unsigned int order);

/* XArray mapping management helpers */
static unsigned long replica_get_original_pfn(struct page *replica_page);
static void replica_remove_mapping(struct page *replica_page, unsigned long pfn_key);
static void replica_restore_mapping(struct page *replica_page, unsigned long pfn_key);

/* Error handling helpers */
static int replica_handle_missing_lru_meta(struct page *replica_page, 
                                          unsigned long pfn_key, 
                                          struct page *head,
                                          const char *operation);

/* LRU management helpers */
static int replica_atomic_lru_remove_and_process(struct page *replica_page,
                                                int (*work_func)(struct page *, unsigned int),
                                                const char *operation);


static void __replica_lru_add_active(struct replica_page_meta *m)
{
    pr_info("[%s] ADD to ACTIVE: head=%p order=%u\n", __func__, m->head, m->order);
    list_add(&m->lru, &replica_active_lru);
}

static void __replica_lru_move_to_active_mru(struct replica_page_meta *m)
{
    pr_info("[%s] MOVE to ACTIVE: head=%p\n", __func__, m->head);
    list_move(&m->lru, &replica_active_lru);
}

static void __replica_lru_move_to_inactive_mru(struct replica_page_meta *m)
{
    pr_info("[%s] MOVE to INACTIVE: head=%p\n", __func__, m->head);
    list_move(&m->lru, &replica_inactive_lru);
}

static void __replica_lru_del(struct replica_page_meta *m)
{
    pr_info("[%s] DEL: head=%p\n", __func__, m->head);
    list_del_init(&m->lru);
    m->on_lru = false;
}

static int replica_lru_insert(struct page *page, unsigned int order)
{
    struct replica_page_meta *m;
    struct page *head = replica_head(page);
    void *ret;

    m = kzalloc(sizeof(*m), GFP_KERNEL);
    if (!m)
        return -ENOMEM;
    m->head = head;
    m->order = order;
    refcount_set(&m->refcount, 1);
    m->on_lru = true;
    INIT_LIST_HEAD(&m->lru);

    spin_lock(&replica_lru_lock);
    ret = xa_store(&replica_meta_xa, (unsigned long)head, m, GFP_ATOMIC);
    if (xa_is_err(ret)) {
        spin_unlock(&replica_lru_lock);
        kfree(m);
        return xa_err(ret);
    }
    __replica_lru_add_active(m);
    spin_unlock(&replica_lru_lock);
    return 0;
}

static bool replica_hw_referenced_locked(struct page *head)
{
    unsigned long vm_flags = 0;
    struct folio *f = replica_folio(head);
    int refs = folio_referenced(f, 1, NULL, &vm_flags);
    
    pr_info("[%s] referenced=%d head=%p\n", __func__, refs, head);
    
    /* If page was referenced, clear the accessed bit for next aging cycle */
    if (refs > 0) {
        folio_clear_referenced(f);
        pr_info("[%s] Cleared accessed bit for referenced page %p\n", __func__, head);
        return true;
    }
    
    return false;
}

/* ========================================================================
 * XArray mapping management helpers
 * ======================================================================== */

/**
 * replica_get_original_pfn - Get original PFN from replica page
 * @replica_page: Replica page to look up
 *
 * Returns: Original PFN key, or 0 if not found
 */
static unsigned long replica_get_original_pfn(struct page *replica_page)
{
    void *original_pfn_val = xa_load(&replica_to_original_xa, (unsigned long)replica_page);
    return original_pfn_val ? xa_to_value(original_pfn_val) : 0;
}

/**
 * replica_remove_mapping - Remove bidirectional mapping between original and replica
 * @replica_page: Replica page
 * @pfn_key: Original PFN key
 */
static void replica_remove_mapping(struct page *replica_page, unsigned long pfn_key)
{
    xa_erase(&original_to_replica_xa, pfn_key);
    xa_erase(&replica_to_original_xa, (unsigned long)replica_page);
}

/**
 * replica_restore_mapping - Restore bidirectional mapping (for error recovery)
 * @replica_page: Replica page
 * @pfn_key: Original PFN key
 */
static void replica_restore_mapping(struct page *replica_page, unsigned long pfn_key)
{
    xa_store(&original_to_replica_xa, pfn_key, replica_page, GFP_ATOMIC);
    xa_store(&replica_to_original_xa, (unsigned long)replica_page, xa_mk_value(pfn_key), GFP_ATOMIC);
}

/**
 * replica_handle_missing_lru_meta - Handle critical error when LRU metadata is missing
 * @replica_page: Replica page
 * @pfn_key: Original PFN key
 * @head: Head page
 * @operation: Operation name for error message
 *
 * This handles the critical situation where mapping XArrays contain the replica
 * but LRU metadata is missing. This should NEVER happen in normal operation.
 */
static int replica_handle_missing_lru_meta(struct page *replica_page, 
                                          unsigned long pfn_key, 
                                          struct page *head,
                                          const char *operation)
{
    /* CRITICAL ERROR: This should NEVER happen in normal operation!
     * If we reach here, it indicates a severe bug in the replica management system:
     * - Memory corruption in LRU metadata XArray
     * - Race condition bypassing our atomic operations
     * - Logic error in replica lifecycle management
     * We restore the mapping to prevent system crash, but this needs investigation.
     */
    replica_restore_mapping(replica_page, pfn_key);

    pr_err("[%s] CRITICAL BUG: Replica page %p (pfn=%lu, head=%p) exists in mapping XArrays but NOT in LRU metadata during %s!\n",
           __func__, replica_page, page_to_pfn(replica_page), head, operation);
    pr_err("[%s] This indicates a serious consistency violation - system integrity compromised!\n", __func__);
    pr_err("[%s] Mapping restored to prevent crash, but immediate investigation required!\n", __func__);

    /* Dump additional debug info */
    pr_err("[%s] Debug: original_pfn_key=%lu, replica_to_original_mapping=%p\n",
           __func__, pfn_key, xa_load(&replica_to_original_xa, (unsigned long)replica_page));
    
    return REPLICA_ERROR_NOENT;
}

/**
 * replica_atomic_lru_remove_and_process - Atomically remove from all XArrays and process
 * @replica_page: Replica page to process
 * @work_func: Work function to execute (unmap_and_free or flush)
 * @operation: Operation name for error messages
 *
 * This function handles the common pattern of:
 * 1. Get original PFN mapping
 * 2. Remove from mapping XArrays 
 * 3. Remove from LRU management
 * 4. Execute work function
 * 5. Handle missing LRU metadata error
 *
 * CONCURRENCY SAFETY:
 * - Mapping XArrays are cleared while holding replica_lru_lock, preventing new lookups
 * - LRU metadata is removed atomically while holding the lock
 * - Work function executes on a page that's no longer discoverable via XArrays
 * - Reference counting prevents premature metadata cleanup during work function execution
 * - If work function fails, all state is restored atomically
 */
static int replica_atomic_lru_remove_and_process(struct page *replica_page,
                                                int (*work_func)(struct page *, unsigned int),
                                                const char *operation)
{
    struct replica_page_meta *meta;
    unsigned long flags;
    unsigned int order = 0;
    struct page *head;
    int ret;
    unsigned long pfn_key;

    if (!replica_page) {
        pr_info("[%s] Invalid replica page for %s\n", __func__, operation);
        return REPLICA_ERROR_INVAL;
    }

    head = replica_head(replica_page);

    /* Remove from both LRU and mapping XArrays atomically */
    spin_lock_irqsave(&replica_lru_lock, flags);
    
    /* Get original PFN mapping first */
    pfn_key = replica_get_original_pfn(replica_page);
    if (!pfn_key) {
        spin_unlock_irqrestore(&replica_lru_lock, flags);
        pr_info("[%s] No original PFN mapping found for %s\n", __func__, operation);
        return REPLICA_ERROR_NOENT;
    }

    /* Remove from mapping XArrays FIRST to block new lookups */
    // save replica_page and pfn_key in case we need to restore mapping

    replica_remove_mapping(replica_page, pfn_key);

    /* Remove from LRU management */
    meta = xa_load(&replica_meta_xa, (unsigned long)head);
    if (meta) {
        order = meta->order;
        /* Get our own reference before removing from LRU */
        refcount_inc(&meta->refcount);
        __replica_lru_del(meta);
        xa_erase(&replica_meta_xa, (unsigned long)head);
        /* Release LRU ownership reference */
        refcount_dec(&meta->refcount);
        spin_unlock_irqrestore(&replica_lru_lock, flags);
        
        /* 
         * Execute work function outside the lock.
         * SAFETY: This is safe because:
         * 1. Page is no longer discoverable via XArrays (removed above)
         * 2. We hold a reference on metadata preventing premature cleanup
         * 3. Work functions only operate on the page itself, not shared structures
         * 4. If work function fails, we restore state atomically below
         */
        ret = work_func(replica_page, order);

        if (ret < 0) {
            struct folio *folio = page_folio(replica_page);
            pr_err("[%s] WORK FUNCTION FAILURE: operation=%s, error=%d\n", 
                   __func__, operation, ret);
            pr_err("[%s] Page details: page=%p, pfn=%lu, order=%u, head=%p\n",
                   __func__, replica_page, page_to_pfn(replica_page), order, head);
            pr_err("[%s] Folio state: refcount=%d, mapcount=%d, dirty=%d, locked=%d\n",
                   __func__, folio_ref_count(folio), folio_mapcount(folio),
                   folio_test_dirty(folio), folio_test_locked(folio));
            pr_err("[%s] Original PFN: %lu\n", __func__, pfn_key);
            
            /* 
             * Restore mapping atomically if work function failed.
             * This ensures the page returns to a consistent state where it can
             * be found via XArrays and properly managed by LRU again.
             */
            spin_lock_irqsave(&replica_lru_lock, flags);

            replica_restore_mapping(replica_page, pfn_key);
            xa_store(&replica_meta_xa, (unsigned long)head, meta, GFP_ATOMIC);
            __replica_lru_add_active(meta);

            spin_unlock_irqrestore(&replica_lru_lock, flags);
            
            pr_info("[%s] Restored page %p to active LRU after %s failure\n",
                   __func__, replica_page, operation);
        }
        
        /* Release our reference (will free meta if we're the last) */
        replica_meta_put_ref(meta);
        return ret;
    } else {
        spin_unlock_irqrestore(&replica_lru_lock, flags);
        return replica_handle_missing_lru_meta(replica_page, pfn_key, head, operation);
    }
}

/* ========================================================================
 * Internal helpers for unified workflow
 * ======================================================================== */

/* Internal helper - XArray removal + mapping unmap + page free */
static int replica_unmap_and_free_page(struct page *replica_page, unsigned int order)
{
    struct folio *folio;
    enum ttu_flags flags = TTU_IGNORE_MLOCK | TTU_BATCH_FLUSH;

    if (!replica_page) {
        return REPLICA_ERROR_INVAL;
    }

    folio = page_folio(replica_page);

    /* Note: XArray mappings should already be removed by caller */

    /* Linux vmscan.c style unmap for mapped folios */
    if (folio_mapped(folio)) {
        /*
         * For large folios, use TTU_SYNC to ensure atomic processing
         * and prevent partial unmap races. We don't need TTU_SPLIT_HUGE_PMD
         * because we're processing the entire replica, not individual subpages.
         */
        if (folio_test_large(folio))
            flags |= TTU_SYNC;

        folio_lock(folio);
        try_to_unmap(folio, flags);
        folio_unlock(folio);
        
        /* Check if still mapped after unmap attempt */
        if (folio_mapped(folio)) {
            pr_err("[%s] Failed to unmap page %p, mapcount=%d\n",
                   __func__, replica_page, folio_mapcount(folio));
            return REPLICA_ERROR_IO;
        }
    }
    
    /* Use unified validation and free function */
    return replica_safe_free_page(replica_page, order, "unmap_and_free");
}

/* ========================================================================
 * Linux-style LRU implementation 
 * ======================================================================== */

/**
 * replica_reclaim_from_inactive - Reclaim pages from inactive list (Linux vmscan style)
 * @nr: Number of pages to attempt to reclaim
 *
 * This function implements Linux-style reclaim from inactive list:
 * - Takes pages from TAIL (LRU) of inactive list
 * - Referenced pages get moved back to active list MRU
 * - Non-referenced pages get unmapped and freed
 */
static unsigned long replica_reclaim_from_inactive(unsigned long nr)
{
    unsigned long flags;
    unsigned long collected = 0, freed = 0;
    struct replica_page_meta *m, *tmp;
    struct list_head process_list;
    
    INIT_LIST_HEAD(&process_list);
    
    /* First pass: collect pages from tail (LRU) of inactive list */
    spin_lock_irqsave(&replica_lru_lock, flags);
    list_for_each_entry_safe_reverse(m, tmp, &replica_inactive_lru, lru) {
        if (collected >= nr)
            break;
        
        /* Get reference and move to processing list */
        if (refcount_inc_not_zero(&m->refcount)) {
            list_move(&m->lru, &process_list);
            m->on_lru = false;
            collected++;
        }
    }
    spin_unlock_irqrestore(&replica_lru_lock, flags);
    
    pr_info("[%s] Collected %lu pages from inactive list for reclaim\n", 
            __func__, collected);
    
    /* Second pass: process pages - check references and reclaim */
    list_for_each_entry_safe(m, tmp, &process_list, lru) {
        struct folio *folio = replica_folio(m->head);
        bool refd, is_dirty;
        int ret;
        unsigned long pfn_key;
        
        /* Check if referenced (last chance for inactive pages) */
        folio_lock(folio);
        refd = replica_hw_referenced_locked(m->head);
        is_dirty = folio_test_dirty(folio);
        folio_unlock(folio);
        
        if (refd) {
            /* Referenced - promote back to active list MRU */
            spin_lock_irqsave(&replica_lru_lock, flags);
            __replica_lru_move_to_active_mru(m);
            m->on_lru = true;
            spin_unlock_irqrestore(&replica_lru_lock, flags);
            replica_meta_put_ref(m);
            pr_info("[%s] Promoted referenced page %p back to active\n", 
                    __func__, m->head);
            continue;
        }
        
        /* Not referenced - proceed with reclaim */
        spin_lock_irqsave(&replica_lru_lock, flags);
        
        /* Get and remove mapping */
        pfn_key = replica_get_original_pfn(m->head);
        if (pfn_key) {
            replica_remove_mapping(m->head, pfn_key);
        }
        
        /* Remove from LRU metadata */
        xa_erase(&replica_meta_xa, (unsigned long)m->head);
        refcount_dec(&m->refcount); /* Release LRU ownership */
        spin_unlock_irqrestore(&replica_lru_lock, flags);
        
        /* Handle dirty vs clean pages */
        if (is_dirty) {
            pr_info("[%s] Reclaiming dirty page %p (pfn=%lu, order=%u) - writeback first\n", 
                    __func__, m->head, page_to_pfn(m->head), m->order);
            ret = replica_page_writeback(m->head);
            if (ret == REPLICA_SUCCESS) {
                ret = replica_unmap_and_free_page(m->head, m->order);
            } else {
                pr_err("[%s] Writeback failed for dirty page %p: %d\n", 
                       __func__, m->head, ret);
            }
        } else {
            pr_info("[%s] Reclaiming clean page %p (pfn=%lu, order=%u) - direct unmap and free\n", 
                    __func__, m->head, page_to_pfn(m->head), m->order);
            ret = replica_unmap_and_free_page(m->head, m->order);
        }
        
        if (ret == REPLICA_SUCCESS) {
            freed++;
            pr_debug("[%s] Successfully reclaimed page %p (pfn=%lu)\n", 
                    __func__, m->head, page_to_pfn(m->head));
        } else {
            struct folio *folio = replica_folio(m->head);
            pr_err("[%s] RECLAIM FAILURE: page=%p, pfn=%lu, order=%u, error=%d\n", 
                   __func__, m->head, page_to_pfn(m->head), m->order, ret);
            pr_err("[%s] Page state: refcount=%d, mapcount=%d, dirty=%d, locked=%d\n",
                   __func__, folio_ref_count(folio), folio_mapcount(folio), 
                   folio_test_dirty(folio), folio_test_locked(folio));
            pr_err("[%s] Original PFN: %lu\n", __func__, pfn_key);
            
            /* Detailed error context */
            if (ret == REPLICA_ERROR_IO) {
                if (folio_mapped(folio)) {
                    pr_err("[%s] Failed to unmap: page still has %d mappings\n",
                           __func__, folio_mapcount(folio));
                } else if (folio_ref_count(folio) > 1) {
                    pr_err("[%s] Failed to free: page still has %d references\n",
                           __func__, folio_ref_count(folio));
                }
            }
            
            /* Restore mapping if unmap failed */
            spin_lock_irqsave(&replica_lru_lock, flags);
            replica_restore_mapping(m->head, pfn_key);
            xa_store(&replica_meta_xa, (unsigned long)m->head, m, GFP_ATOMIC);
            list_add_tail(&m->lru, &replica_inactive_lru);
            m->on_lru = true;
            spin_unlock_irqrestore(&replica_lru_lock, flags);
            
            pr_info("[%s] Restored page %p to inactive list after reclaim failure\n",
                   __func__, m->head);
        }
        
        replica_meta_put_ref(m);
    }
    
    pr_info("[%s] Reclaimed %lu pages from inactive list\n", __func__, freed);
    return freed;
}

/**
 * replica_age_active_to_inactive - Age pages from active to inactive list
 * @nr: Number of pages to scan for aging
 *
 * Linux-style aging:
 * - Takes pages from TAIL (LRU) of active list  
 * - Referenced pages stay in active list MRU
 * - Non-referenced pages move to inactive list MRU
 */
static unsigned int replica_age_active_to_inactive(unsigned long nr)
{
    unsigned long flags;
    unsigned long collected = 0;
    unsigned int aged = 0;
    struct replica_page_meta *m, *tmp;
    struct list_head process_list;
    
    INIT_LIST_HEAD(&process_list);
    
    /* First pass: collect pages from tail (LRU) of active list */
    spin_lock_irqsave(&replica_lru_lock, flags);
    list_for_each_entry_safe_reverse(m, tmp, &replica_active_lru, lru) {
        if (collected >= nr)
            break;
        
        /* Get reference and move to processing list */
        if (refcount_inc_not_zero(&m->refcount)) {
            list_move(&m->lru, &process_list);
            m->on_lru = false;
            collected++;
        }
    }
    spin_unlock_irqrestore(&replica_lru_lock, flags);
    
    pr_info("[%s] Collected %lu pages from active list for aging\n", 
            __func__, collected);
    
    /* Second pass: check references and age appropriately */
    list_for_each_entry_safe(m, tmp, &process_list, lru) {
        struct folio *folio = replica_folio(m->head);
        bool refd;
        
        /* Check hardware reference bit */
        folio_lock(folio);
        refd = replica_hw_referenced_locked(m->head);
        folio_unlock(folio);
        
        spin_lock_irqsave(&replica_lru_lock, flags);
        
        if (refd) {
            /* Still referenced - keep in active list MRU */
            __replica_lru_move_to_active_mru(m);
            m->on_lru = true;
            pr_info("[%s] Keeping referenced page %p in active\n", 
                    __func__, m->head);
        } else {
            /* Not referenced - move to inactive list MRU */
            __replica_lru_move_to_inactive_mru(m);
            m->on_lru = true;
            aged++;
            pr_info("[%s] Aged page %p to inactive\n", __func__, m->head);
        }
        
        spin_unlock_irqrestore(&replica_lru_lock, flags);
        replica_meta_put_ref(m);
    }
    
    pr_info("[%s] Aged %u pages from active to inactive\n", __func__, aged);
    return aged;
}

/* ========================================================================
 * Shrinker integration (single, final)
 * ======================================================================== */

static unsigned long __replica_list_len(struct list_head *head)
{
    unsigned long n = 0;
    struct replica_page_meta *m;
    list_for_each_entry(m, head, lru) {
        if (++n > REPLICA_MAX_LIST_COUNT)
            break;
    }
    return n;
}

static unsigned long replica_shrink_count(struct shrinker *s,
                                          struct shrink_control *sc)
{
    unsigned long flags, n;
    spin_lock_irqsave(&replica_lru_lock, flags);
    n  = __replica_list_len(&replica_inactive_lru);
    n += __replica_list_len(&replica_active_lru) >> REPLICA_ACTIVE_TO_INACTIVE_RATIO;
    spin_unlock_irqrestore(&replica_lru_lock, flags);

    pr_info("[%s] shrink_count: returning %lu pages\n", __func__, n);
    return n;
}

static unsigned long replica_shrink_scan(struct shrink_control *sc)
{
    unsigned long nr_to_scan = sc->nr_to_scan ? sc->nr_to_scan : REPLICA_DEFAULT_SCAN_PAGES;
    unsigned long flags;
    unsigned long inactive_len, freed = 0;
    
    pr_info("[%s] shrink_scan: nr_to_scan=%lu\n", __func__, nr_to_scan);
    
    /* Step 1: Check if inactive list has enough pages for direct reclaim */
    spin_lock_irqsave(&replica_lru_lock, flags);
    inactive_len = __replica_list_len(&replica_inactive_lru);
    spin_unlock_irqrestore(&replica_lru_lock, flags);
    
    if (inactive_len >= nr_to_scan * REPLICA_INACTIVE_THRESHOLD_MULT) {
        /* Step 1-1: Direct reclaim from inactive list */
        freed = replica_reclaim_from_inactive(nr_to_scan);
        pr_info("[%s] Direct reclaim: inactive_len=%lu, freed=%lu\n", 
                __func__, inactive_len, freed);
        return freed;
    }
    
    /* Step 2: Not enough inactive pages, need to age active pages first */
    pr_info("[%s] Not enough inactive pages (%lu < %lu), aging active pages\n",
            __func__, inactive_len, nr_to_scan * REPLICA_INACTIVE_THRESHOLD_MULT);
    
    unsigned int aged = replica_age_active_to_inactive(nr_to_scan * REPLICA_AGING_MULT);
    
    /* Step 3: Try reclaim again after aging */
    spin_lock_irqsave(&replica_lru_lock, flags);
    inactive_len = __replica_list_len(&replica_inactive_lru);
    spin_unlock_irqrestore(&replica_lru_lock, flags);
    
    if (inactive_len >= nr_to_scan * REPLICA_INACTIVE_THRESHOLD_MULT) {
        freed = replica_reclaim_from_inactive(nr_to_scan);
    }
    
    pr_info("[%s] Final result: aged=%u, inactive_len=%lu, freed=%lu\n",
            __func__, aged, inactive_len, freed);
    
    return freed;
}

static unsigned long replica_shrink_scan_wrapper(struct shrinker *s,
                                                struct shrink_control *sc)
{
    return replica_shrink_scan(sc);
}

static struct shrinker *replica_shrinker;

/* Manual shrinker trigger function */
static void replica_trigger_shrink(unsigned long nr_to_free)
{
    struct shrink_control sc = {
        .nr_to_scan = nr_to_free,
        .gfp_mask = GFP_KERNEL,
    };
    
    unsigned long freed = replica_shrink_scan(&sc);
    pr_info("[%s] Manual shrink: requested=%lu, freed=%lu\n",
            __func__, nr_to_free, freed);
}

static int __init replica_shrinker_init(void)
{
    replica_shrinker = shrinker_alloc(0, "replica_shrinker");
    if (!replica_shrinker) {
        pr_err("[%s] failed to allocate shrinker\n", __func__);
        return -ENOMEM;
    }

    replica_shrinker->count_objects = replica_shrink_count;
    replica_shrinker->scan_objects = replica_shrink_scan_wrapper;
    replica_shrinker->seeks = DEFAULT_SEEKS;

    shrinker_register(replica_shrinker);
    pr_info("[%s] shrinker registered\n", __func__);
    return 0;
}

subsys_initcall(replica_shrinker_init);


/* ========================================================================
 * Public API for page coherence integration
 * ======================================================================== */

/* Helper to validate parameters for replica page creation */
static int replica_validate_create_params(void *src_kaddr, size_t size)
{
    if (!src_kaddr) {
        pr_err("[%s] NULL source address\n", __func__);
        return REPLICA_ERROR_INVAL;
    }
    
    if (!REPLICA_SIZE_IS_VALID(size)) {
        pr_err("[%s] Invalid size: %zu (expected %lu or %lu)\n", 
               __func__, size, PAGE_SIZE, PMD_SIZE);
        return REPLICA_ERROR_INVAL;
    }
    
    return REPLICA_SUCCESS;
}

/* Helper to check if replica already exists */
static int replica_check_duplicate(unsigned long pfn_key)
{
    if (xa_load(&original_to_replica_xa, pfn_key)) {
        pr_info("[%s] Replica already exists for pfn %lu\n", __func__, pfn_key);
        return REPLICA_ERROR_EXIST;
    }
    return REPLICA_SUCCESS;
}

/* Helper to allocate page with retry and shrinking */
static struct page *replica_allocate_with_retry(unsigned int order)
{
    struct page *replica_page;
    gfp_t gfp_flags = GFP_HIGHUSER_MOVABLE | __GFP_ZERO;
    int retry_count = 0;
    
    if (order > 0)
        gfp_flags |= __GFP_COMP;

retry_alloc:
    replica_page = alloc_pages(gfp_flags, order);
    if (!replica_page) {
        if (retry_count < REPLICA_MAX_RETRIES) {
            /* Calculate how many pages to free */
            unsigned long pages_to_free = (order == 0) ? 64 : 8;
            pages_to_free = max(32UL, pages_to_free);

            pr_info("[%s] Allocation failed (retry %d/%d), triggering manual shrink of %lu pages\n",
                    __func__, retry_count + 1, REPLICA_MAX_RETRIES, pages_to_free);

            replica_trigger_shrink(pages_to_free);
            msleep(10);  /* Brief delay for shrinking to complete */
            
            retry_count++;
            goto retry_alloc;
        }

        pr_err("[%s] Failed to allocate replica page after %d retries (order=%u)\n",
               __func__, REPLICA_MAX_RETRIES, order);
        return NULL;
    }

    /* CRITICAL: Verify compound page structure after allocation */
    if (order > 0) {
        if (WARN_ON(!PageCompound(replica_page))) {
            pr_err("[%s] CRITICAL: Allocated page %p order=%u is not compound!\n",
                   __func__, replica_page, order);
            __free_pages(replica_page, order);
            return NULL;
        }
        
        /* Verify tail pages are properly set up */
        unsigned int i;
        for (i = 1; i < (1U << order); i++) {
            struct page *tail = replica_page + i;
            if (WARN_ON(!PageTail(tail) || compound_head(tail) != replica_page)) {
                pr_err("[%s] CRITICAL: Tail page %u (page %p) not properly linked to head %p!\n",
                       __func__, i, tail, replica_page);
                __free_pages(replica_page, order);
                return NULL;
            }
        }
        
        pr_info("[%s] Successfully allocated compound page %p order=%u with %u pages\n",
                __func__, replica_page, order, 1U << order);
    }

    if (retry_count > 0) {
        pr_info("[%s] Allocation succeeded after %d retries and manual shrinking\n", 
                __func__, retry_count);
    }
    
    return replica_page;
}

/* Helper to establish bidirectional mapping */
static int replica_establish_mapping(struct page *replica_page, unsigned long pfn_key)
{
    void *xa_ret;
    int err;
    
    xa_ret = xa_store(&original_to_replica_xa, pfn_key, replica_page, GFP_KERNEL);
    if (xa_is_err(xa_ret)) {
        err = xa_err(xa_ret);
        pr_info("[%s] Failed to store original->replica mapping: %d\n", __func__, err);
        return REPLICA_ERROR_IO;
    }
    
    xa_ret = xa_store(&replica_to_original_xa, (unsigned long)replica_page, 
                     xa_mk_value(pfn_key), GFP_KERNEL);
    if (xa_is_err(xa_ret)) {
        err = xa_err(xa_ret);
        xa_erase(&original_to_replica_xa, pfn_key);
        pr_info("[%s] Failed to store replica->original mapping: %d\n", __func__, err);
        return REPLICA_ERROR_IO;
    }
    
    return REPLICA_SUCCESS;
}

/* Helper to remove page from LRU during error cleanup */
static void replica_lru_remove_on_error(struct page *page)
{
    struct replica_page_meta *meta;
    struct page *head = replica_head(page);
    unsigned long flags;
    
    spin_lock_irqsave(&replica_lru_lock, flags);
    meta = xa_load(&replica_meta_xa, (unsigned long)head);
    if (meta) {
        __replica_lru_del(meta);
        xa_erase(&replica_meta_xa, (unsigned long)head);
        pr_info("[%s] Removed page %p from LRU during error cleanup\n", __func__, head);
    }
    spin_unlock_irqrestore(&replica_lru_lock, flags);
    
    /* Free the metadata */
    if (meta) {
        kfree(meta);
    }
}

/**
 * replica_page_create - Create a new replica page
 * @order: Page order (0 for single page, PMD_ORDER for huge page)
 * @original_pfn: Original page PFN to replicate
 * @src_kaddr: Source kernel virtual address for data copy
 * @size: Size to copy (PAGE_SIZE or PMD_SIZE)
 *
 * Creates a new replica page, adds it to LRU management, and establishes
 * bidirectional mapping with the original page.
 *
 * Returns: Replica page pointer on success, ERR_PTR on failure
 */
struct page *replica_page_create(unsigned int order, pfn_t original_pfn, 
                                void *src_kaddr, size_t size)
{
    struct page *replica_page;
    struct page_replication_info *replica_info;
    unsigned long pfn_key = pfn_t_to_pfn(original_pfn);
    int err;

    /* Validate input parameters */
    err = replica_validate_create_params(src_kaddr, size);
    if (err != REPLICA_SUCCESS)
        return ERR_PTR(err);

    /* Check if replica already exists */
    err = replica_check_duplicate(pfn_key);
    if (err != REPLICA_SUCCESS)
        return ERR_PTR(err);

    /* Step 1: Allocate replica page with retry and manual shrinking */
    replica_page = replica_allocate_with_retry(order);
    if (!replica_page)
        return ERR_PTR(REPLICA_ERROR_NOMEM);

    /* Step 2: Copy data from source to replica using unified helper */
    void *dst_kaddr = replica_map_pages_safe(replica_page, size, "page_create");
    if (!dst_kaddr) {
        pr_err("[%s] Failed to map replica page for copy\n", __func__);
        err = REPLICA_ERROR_IO;
        goto free_page;
    }
    
    err = replica_copy_data(src_kaddr, dst_kaddr, size);
    replica_unmap_pages_safe(dst_kaddr, replica_page, size);
    
    if (err != REPLICA_SUCCESS) {
        pr_err("[%s] Data copy failed: %d\n", __func__, err);
        goto free_page;
    }

    /* Step 3: Mark pages as up to date */
    mark_compound_uptodate(replica_page, order);

    /* Step 4: Add to LRU management */
    err = replica_lru_insert(replica_page, order);
    if (err) {
        pr_err("[%s] LRU insertion failed: %d\n", __func__, err);
        goto free_page;
    }

    /* Step 5: Establish bidirectional mapping */
    err = replica_establish_mapping(replica_page, pfn_key);
    if (err) {
        pr_err("[%s] Mapping establishment failed: %d\n", __func__, err);
        goto remove_from_lru;
    }

    /* Step 6: Setup page replication info if available */
    replica_info = get_page_replication_info(replica_page);
    if (replica_info) {
        replica_info->original_pfn = original_pfn;
        set_page_replication_info(replica_page, replica_info);
    }

    pr_info("[%s] Created replica page (order=%u, pfn=%lu, original_pfn=%lu)\n",
            __func__, order, page_to_pfn(replica_page), pfn_key);

    return replica_page;

remove_from_lru:
    /* Step 5 failed: Remove from LRU and XArray */
    replica_lru_remove_on_error(replica_page);
    pr_err("[%s] Removed page from LRU due to mapping failure\n", __func__);
    /* Fall through to free_page */

free_page:
    /* Step 1-4 failed: Free the allocated page */
    __free_pages(replica_page, order);
    pr_err("[%s] Failed to create replica page: %d\n", __func__, err);
    return ERR_PTR(err);
}
EXPORT_SYMBOL(replica_page_create);

/**
 * replica_page_find - Find existing replica page by original PFN
 * @original_pfn: Original page PFN to look up
 *
 * Returns: Replica page pointer if found, NULL if not found
 * 
 * Note: Caller must call replica_page_put() when done with the returned page
 * to release the reference and avoid memory leaks.
 */
struct page *replica_page_find(pfn_t original_pfn)
{
    unsigned long pfn_key = pfn_t_to_pfn(original_pfn);
    struct page *replica_page;
    struct replica_page_meta *meta;
    struct page *head;
    
    /* First: get replica page pointer (may become stale) */
    replica_page = xa_load(&original_to_replica_xa, pfn_key);
    if (!replica_page)
        return NULL;
    
    head = replica_head(replica_page);
    
    /* Second: get reference to prevent freeing using unified helper */
    meta = replica_get_meta_with_ref(head);
    if (meta) {
        pr_info("[%s] Found replica page %p for original pfn %lu (with reference)\n", 
                __func__, replica_page, pfn_key);
        return replica_page;
    } else {
        /* Failed: page was freed or being freed */
        return NULL;
    }
}
EXPORT_SYMBOL(replica_page_find);

/**
 * replica_page_put - Release reference obtained from replica_page_find()
 * @replica_page: Replica page to release reference for
 *
 * This function must be called for every replica page returned by
 * replica_page_find() to release the reference and prevent memory leaks.
 */
void replica_page_put(struct page *replica_page)
{
    struct replica_page_meta *meta;
    struct page *head;
    
    if (!replica_page)
        return;
    
    head = replica_head(replica_page);
    
    /* 
     * We need to release the reference from replica_page_find().
     * Instead of using replica_get_meta_with_ref() which adds a temporary reference
     * and then calling put_ref twice (which creates a race condition), we directly
     * access the metadata under lock and decrement only once.
     */
    unsigned long flags;
    spin_lock_irqsave(&replica_lru_lock, flags);
    meta = xa_load(&replica_meta_xa, (unsigned long)head);
    if (meta) {
        /* Decrement the reference from replica_page_find() */
        replica_meta_put_ref(meta);
    }
    spin_unlock_irqrestore(&replica_lru_lock, flags);
    /* If meta is NULL, it was already freed or being freed, nothing to do */
}
EXPORT_SYMBOL(replica_page_put);

/**
 * replica_page_unmap_and_free - Unmap and free a replica page
 * @replica_page: Replica page to be unmapped and freed
 *
 * Returns: 0 on success, negative error code on failure
 */
int replica_page_unmap_and_free(struct page *replica_page)
{
    return replica_atomic_lru_remove_and_process(replica_page, 
                                                replica_unmap_and_free_page,
                                                "unmap_and_free");
}
EXPORT_SYMBOL(replica_page_unmap_and_free);

/**
 * replica_page_writeback - Write replica data back to original page and clear dirty bit
 * @replica_page: Replica page to writeback
 *
 * Returns: 0 on success, negative error code on failure
 */
int replica_page_writeback(struct page *replica_page)
{
    struct folio *folio;
    struct page *original_page;
    unsigned int order;
    size_t copy_size;
    int ret;
    
    if (!replica_page) {
        pr_info("[%s] Invalid replica page for writeback\n", __func__);
        return REPLICA_ERROR_INVAL;
    }

    folio = page_folio(replica_page);
    order = compound_order(replica_page);
    copy_size = PAGE_SIZE << order;

    /* Validate compound page structure before writeback */
    ret = replica_validate_compound_page(replica_page, order);
    if (ret != REPLICA_SUCCESS) {
        pr_err("[%s] Replica page structure validation failed: %d\n", __func__, ret);
        return ret;
    }

    /* Get original PFN from replica->original mapping */
    unsigned long pfn_key = replica_get_original_pfn(replica_page);
    if (!pfn_key) {
        pr_info("[%s] No original PFN mapping found for replica page %p\n", __func__, replica_page);
        return REPLICA_ERROR_NOENT;
    }
    original_page = pfn_to_page(pfn_key);
    
    /* Validate original page structure */
    ret = replica_validate_compound_page(original_page, order);
    if (ret != REPLICA_SUCCESS) {
        pr_err("[%s] Original page structure validation failed: %d\n", __func__, ret);
        return ret;
    }
    
    /* Map both replica and original pages safely for copying */
    void *src_kaddr = replica_map_pages_safe(replica_page, copy_size, "writeback_src");
    if (!src_kaddr) {
        pr_err("[%s] Failed to map replica page for writeback\n", __func__);
        return REPLICA_ERROR_IO;
    }
    
    void *dst_kaddr = replica_map_pages_safe(original_page, copy_size, "writeback_dst");
    if (!dst_kaddr) {
        pr_err("[%s] Failed to map original page for writeback\n", __func__);
        replica_unmap_pages_safe(src_kaddr, replica_page, copy_size);
        return REPLICA_ERROR_IO;
    }
    
    /* Copy data from replica to original using simple method */
    ret = replica_copy_data(src_kaddr, dst_kaddr, copy_size);
    
    /* Unmap both pages */
    replica_unmap_pages_safe(dst_kaddr, original_page, copy_size);
    replica_unmap_pages_safe(src_kaddr, replica_page, copy_size);
    
    if (ret != REPLICA_SUCCESS) {
        return ret;
    }
    
    /* Clear dirty bit after successful writeback */
    if (folio_test_dirty(folio)) {
        folio_clear_dirty(folio);
    }

    pr_info("[%s] Wrote back replica page %p to original pfn %lu\n",
            __func__, replica_page, pfn_key);

    return REPLICA_SUCCESS;
}
EXPORT_SYMBOL(replica_page_writeback);

/* Special work function for flush operation (writeback + unmap_and_free) */
static int replica_flush_page(struct page *replica_page, unsigned int order)
{
    int ret;
    
    /* Step 1: Writeback (replica -> original data copy) */
    ret = replica_page_writeback(replica_page);
    if (ret == REPLICA_SUCCESS) {
        /* Step 2: Unmap and free (no more XArray operations) */
        ret = replica_unmap_and_free_page(replica_page, order);
    }
    
    return ret;
}

/**
 * replica_page_flush - Flush and free a replica page (flush = writeback + unmap_and_free)
 * @replica_page: Replica page to flush
 *
 * Returns: 0 on success, negative error code on failure
 */
int replica_page_flush(struct page *replica_page)
{
    return replica_atomic_lru_remove_and_process(replica_page, 
                                                replica_flush_page,
                                                "flush");
}
EXPORT_SYMBOL(replica_page_flush);

/**
 * replica_page_write_protect - Remove write permissions from all mappings of a replica page
 * @replica_page: Replica page to write-protect
 *
 * This function removes write permissions from all PTE/PMD entries that map
 * the replica page, effectively making it read-only. This is useful for
 * Mâ†’S (Modified to Shared) state transitions in coherence protocols.
 *
 * Returns: Number of mappings that were write-protected, negative error code on failure
 */
int replica_page_write_protect(struct page *replica_page)
{
    struct folio *folio;
    int cleaned;
    
    if (!replica_page) {
        pr_info("[%s] Invalid replica page for write protection\n", __func__);
        return REPLICA_ERROR_INVAL;
    }
    
    if (!page_mapped(replica_page)) {
        pr_info("[%s] Replica page %p not mapped, no protection needed\n", __func__, replica_page);
        return 0;
    }
    
    folio = page_folio(replica_page);
    folio_lock(folio);
    
    /* Remove write permission from all mappings using folio_mkclean */
    cleaned = folio_mkclean(folio);
    
    folio_unlock(folio);

    pr_info("[%s] Write-protected %d mappings for replica page %p\n",
            __func__, cleaned, replica_page);

    return cleaned;
}
EXPORT_SYMBOL(replica_page_write_protect);
